{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, models, transforms\n",
    "import os\n",
    "import math\n",
    "import fnmatch\n",
    "import nets\n",
    "import utils\n",
    "import training_functions\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(activations=False, batch_size=256, bias=True, custom_img_size=[128, 128, 3], dataset='MNIST-train', dataset_path='data', epochs=1000, epochs_pretrain=20, gamma=0.1, leaky=True, mode='train_full', neg_slope=0.01, net_architecture='CAE_3', num_clusters=10, pretrain=True, pretrained_net=1, printing_frequency=10, rate=0.001, rate_pretrain=0.001, sched_gamma=0.1, sched_gamma_pretrain=0.1, sched_step=200, sched_step_pretrain=200, tensorboard=True, tol=0.01, update_interval=80, weight=0.0, weight_pretrain=0.0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Translate string entries to bool for parser\n",
    "def str2bool(v):\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Use DCEC for clustering')\n",
    "parser.add_argument('--mode', default='train_full', choices=['train_full', 'pretrain'], help='mode')\n",
    "parser.add_argument('--tensorboard', default=True, type=bool, help='export training stats to tensorboard')\n",
    "parser.add_argument('--pretrain', default=True, type=str2bool, help='perform autoencoder pretraining')\n",
    "parser.add_argument('--pretrained_net', default=1, help='index or path of pretrained net')\n",
    "parser.add_argument('--net_architecture', default='CAE_3', choices=['CAE_3', 'CAE_bn3', 'CAE_4', 'CAE_bn4', 'CAE_5', 'CAE_bn5'], help='network architecture used')\n",
    "parser.add_argument('--dataset', default='MNIST-train',\n",
    "                    choices=['MNIST-train', 'custom', 'MNIST-test', 'MNIST-full'],\n",
    "                    help='custom or prepared dataset')\n",
    "parser.add_argument('--dataset_path', default='data', help='path to dataset')\n",
    "parser.add_argument('--batch_size', default=256, type=int, help='batch size')\n",
    "parser.add_argument('--rate', default=0.001, type=float, help='learning rate for clustering')\n",
    "parser.add_argument('--rate_pretrain', default=0.001, type=float, help='learning rate for pretraining')\n",
    "parser.add_argument('--weight', default=0.0, type=float, help='weight decay for clustering')\n",
    "parser.add_argument('--weight_pretrain', default=0.0, type=float, help='weight decay for clustering')\n",
    "parser.add_argument('--sched_step', default=200, type=int, help='scheduler steps for rate update')\n",
    "parser.add_argument('--sched_step_pretrain', default=200, type=int,\n",
    "                    help='scheduler steps for rate update - pretrain')\n",
    "parser.add_argument('--sched_gamma', default=0.1, type=float, help='scheduler gamma for rate update')\n",
    "parser.add_argument('--sched_gamma_pretrain', default=0.1, type=float,\n",
    "                    help='scheduler gamma for rate update - pretrain')\n",
    "parser.add_argument('--epochs', default=1000, type=int, help='clustering epochs')\n",
    "parser.add_argument('--epochs_pretrain', default=20, type=int, help='pretraining epochs')\n",
    "parser.add_argument('--printing_frequency', default=10, type=int, help='training stats printing frequency')\n",
    "parser.add_argument('--gamma', default=0.1, type=float, help='clustering loss weight')\n",
    "parser.add_argument('--update_interval', default=80, type=int, help='update interval for target distribution')\n",
    "parser.add_argument('--tol', default=1e-2, type=float, help='stop criterium tolerance')\n",
    "parser.add_argument('--num_clusters', default=10, type=int, help='number of clusters')\n",
    "parser.add_argument('--custom_img_size', default=[128, 128, 3], nargs=3, type=int, help='size of custom images')\n",
    "parser.add_argument('--leaky', default=True, type=str2bool)\n",
    "parser.add_argument('--neg_slope', default=0.01, type=float)\n",
    "parser.add_argument('--activations', default=False, type=str2bool)\n",
    "parser.add_argument('--bias', default=True, type=str2bool)\n",
    "args = parser.parse_args(\"\")\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_loss(output):\n",
    "    # Nu: # of unlabeled data\n",
    "    # C: number of clusters we have so far\n",
    "    # 1/Nu * sum\n",
    "    # Output are clustering scores for unlabeled data\n",
    "    Nu = len(output)\n",
    "    loss = Nu * torch.sum(output*torch.log(output))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the 'CAE_3' architecture\n",
      "\n",
      "The following parameters are used:\n",
      "Batch size:\t256\n",
      "Number of workers:\t4\n",
      "Learning rate:\t0.001\n",
      "Pretraining learning rate:\t0.001\n",
      "Weight decay:\t0.0\n",
      "Pretraining weight decay:\t0.0\n",
      "Scheduler steps:\t200\n",
      "Scheduler gamma:\t0.1\n",
      "Pretraining scheduler steps:\t200\n",
      "Pretraining scheduler gamma:\t0.1\n",
      "Number of epochs of training:\t1000\n",
      "Number of epochs of pretraining:\t20\n",
      "Clustering loss weight:\t0.1\n",
      "Update interval for target distribution:\t80\n",
      "Stop criterium tolerance:\t0.01\n",
      "Number of clusters:\t10\n",
      "Leaky relu:\tTrue\n",
      "Leaky slope:\t0.01\n",
      "Activations:\tFalse\n",
      "Bias:\tTrue\n",
      "\n",
      "Data preparation\n",
      "Reading data from: MNIST train dataset\n",
      "Image size used:\t28x28\n",
      "Training set size:\t60000\n",
      "\n",
      "Performing calculations on:\tcuda:0\n",
      "\n",
      "Pretraining:\tEpoch 1/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining:\tEpoch: [1][10/235]\tLoss 0.0858 (0.1020)\t\n",
      "Pretraining:\tEpoch: [1][20/235]\tLoss 0.0753 (0.0901)\t\n",
      "Pretraining:\tEpoch: [1][30/235]\tLoss 0.0630 (0.0832)\t\n",
      "Pretraining:\tEpoch: [1][40/235]\tLoss 0.0675 (0.0794)\t\n",
      "Pretraining:\tEpoch: [1][50/235]\tLoss 0.0667 (0.0767)\t\n",
      "Pretraining:\tEpoch: [1][60/235]\tLoss 0.0609 (0.0748)\t\n",
      "Pretraining:\tEpoch: [1][70/235]\tLoss 0.0575 (0.0729)\t\n",
      "Pretraining:\tEpoch: [1][80/235]\tLoss 0.0590 (0.0715)\t\n",
      "Pretraining:\tEpoch: [1][90/235]\tLoss 0.0574 (0.0700)\t\n",
      "Pretraining:\tEpoch: [1][100/235]\tLoss 0.0507 (0.0682)\t\n",
      "Pretraining:\tEpoch: [1][110/235]\tLoss 0.0443 (0.0663)\t\n",
      "Pretraining:\tEpoch: [1][120/235]\tLoss 0.0413 (0.0645)\t\n",
      "Pretraining:\tEpoch: [1][130/235]\tLoss 0.0405 (0.0627)\t\n",
      "Pretraining:\tEpoch: [1][140/235]\tLoss 0.0365 (0.0609)\t\n",
      "Pretraining:\tEpoch: [1][150/235]\tLoss 0.0356 (0.0593)\t\n",
      "Pretraining:\tEpoch: [1][160/235]\tLoss 0.0321 (0.0577)\t\n",
      "Pretraining:\tEpoch: [1][170/235]\tLoss 0.0306 (0.0562)\t\n",
      "Pretraining:\tEpoch: [1][180/235]\tLoss 0.0302 (0.0548)\t\n",
      "Pretraining:\tEpoch: [1][190/235]\tLoss 0.0298 (0.0535)\t\n",
      "Pretraining:\tEpoch: [1][200/235]\tLoss 0.0277 (0.0522)\t\n",
      "Pretraining:\tEpoch: [1][210/235]\tLoss 0.0277 (0.0511)\t\n",
      "Pretraining:\tEpoch: [1][220/235]\tLoss 0.0263 (0.0499)\t\n",
      "Pretraining:\tEpoch: [1][230/235]\tLoss 0.0254 (0.0489)\t\n",
      "Pretraining:\t Loss: 0.0485\n",
      "\n",
      "Pretraining:\tEpoch 2/20\n",
      "----------\n",
      "Pretraining:\tEpoch: [2][10/235]\tLoss 0.0244 (0.0252)\t\n",
      "Pretraining:\tEpoch: [2][20/235]\tLoss 0.0243 (0.0250)\t\n",
      "Pretraining:\tEpoch: [2][30/235]\tLoss 0.0238 (0.0247)\t\n",
      "Pretraining:\tEpoch: [2][40/235]\tLoss 0.0237 (0.0245)\t\n",
      "Pretraining:\tEpoch: [2][50/235]\tLoss 0.0259 (0.0243)\t\n",
      "Pretraining:\tEpoch: [2][60/235]\tLoss 0.0235 (0.0242)\t\n",
      "Pretraining:\tEpoch: [2][70/235]\tLoss 0.0223 (0.0241)\t\n",
      "Pretraining:\tEpoch: [2][80/235]\tLoss 0.0212 (0.0239)\t\n",
      "Pretraining:\tEpoch: [2][90/235]\tLoss 0.0224 (0.0238)\t\n",
      "Pretraining:\tEpoch: [2][100/235]\tLoss 0.0203 (0.0236)\t\n",
      "Pretraining:\tEpoch: [2][110/235]\tLoss 0.0204 (0.0234)\t\n",
      "Pretraining:\tEpoch: [2][120/235]\tLoss 0.0215 (0.0233)\t\n",
      "Pretraining:\tEpoch: [2][130/235]\tLoss 0.0207 (0.0232)\t\n",
      "Pretraining:\tEpoch: [2][140/235]\tLoss 0.0205 (0.0230)\t\n",
      "Pretraining:\tEpoch: [2][150/235]\tLoss 0.0217 (0.0230)\t\n",
      "Pretraining:\tEpoch: [2][160/235]\tLoss 0.0197 (0.0228)\t\n",
      "Pretraining:\tEpoch: [2][170/235]\tLoss 0.0197 (0.0227)\t\n",
      "Pretraining:\tEpoch: [2][180/235]\tLoss 0.0214 (0.0226)\t\n",
      "Pretraining:\tEpoch: [2][190/235]\tLoss 0.0211 (0.0225)\t\n",
      "Pretraining:\tEpoch: [2][200/235]\tLoss 0.0196 (0.0224)\t\n",
      "Pretraining:\tEpoch: [2][210/235]\tLoss 0.0201 (0.0223)\t\n",
      "Pretraining:\tEpoch: [2][220/235]\tLoss 0.0198 (0.0222)\t\n",
      "Pretraining:\tEpoch: [2][230/235]\tLoss 0.0195 (0.0221)\t\n",
      "Pretraining:\t Loss: 0.0220\n",
      "\n",
      "Pretraining:\tEpoch 3/20\n",
      "----------\n",
      "Pretraining:\tEpoch: [3][10/235]\tLoss 0.0181 (0.0193)\t\n",
      "Pretraining:\tEpoch: [3][20/235]\tLoss 0.0191 (0.0193)\t\n",
      "Pretraining:\tEpoch: [3][30/235]\tLoss 0.0188 (0.0192)\t\n",
      "Pretraining:\tEpoch: [3][40/235]\tLoss 0.0194 (0.0193)\t\n",
      "Pretraining:\tEpoch: [3][50/235]\tLoss 0.0206 (0.0192)\t\n",
      "Pretraining:\tEpoch: [3][60/235]\tLoss 0.0193 (0.0193)\t\n",
      "Pretraining:\tEpoch: [3][70/235]\tLoss 0.0185 (0.0193)\t\n",
      "Pretraining:\tEpoch: [3][80/235]\tLoss 0.0171 (0.0192)\t\n",
      "Pretraining:\tEpoch: [3][90/235]\tLoss 0.0196 (0.0192)\t\n",
      "Pretraining:\tEpoch: [3][100/235]\tLoss 0.0176 (0.0191)\t\n",
      "Pretraining:\tEpoch: [3][110/235]\tLoss 0.0176 (0.0191)\t\n",
      "Pretraining:\tEpoch: [3][120/235]\tLoss 0.0186 (0.0191)\t\n",
      "Pretraining:\tEpoch: [3][130/235]\tLoss 0.0179 (0.0191)\t\n",
      "Pretraining:\tEpoch: [3][140/235]\tLoss 0.0176 (0.0190)\t\n",
      "Pretraining:\tEpoch: [3][150/235]\tLoss 0.0192 (0.0190)\t\n",
      "Pretraining:\tEpoch: [3][160/235]\tLoss 0.0168 (0.0189)\t\n",
      "Pretraining:\tEpoch: [3][170/235]\tLoss 0.0172 (0.0189)\t\n",
      "Pretraining:\tEpoch: [3][180/235]\tLoss 0.0189 (0.0189)\t\n",
      "Pretraining:\tEpoch: [3][190/235]\tLoss 0.0185 (0.0188)\t\n",
      "Pretraining:\tEpoch: [3][200/235]\tLoss 0.0171 (0.0188)\t\n",
      "Pretraining:\tEpoch: [3][210/235]\tLoss 0.0177 (0.0187)\t\n",
      "Pretraining:\tEpoch: [3][220/235]\tLoss 0.0177 (0.0187)\t\n",
      "Pretraining:\tEpoch: [3][230/235]\tLoss 0.0175 (0.0186)\t\n",
      "Pretraining:\t Loss: 0.0186\n",
      "\n",
      "Pretraining:\tEpoch 4/20\n",
      "----------\n",
      "Pretraining:\tEpoch: [4][10/235]\tLoss 0.0161 (0.0172)\t\n",
      "Pretraining:\tEpoch: [4][20/235]\tLoss 0.0174 (0.0173)\t\n",
      "Pretraining:\tEpoch: [4][30/235]\tLoss 0.0168 (0.0173)\t\n",
      "Pretraining:\tEpoch: [4][40/235]\tLoss 0.0175 (0.0174)\t\n",
      "Pretraining:\tEpoch: [4][50/235]\tLoss 0.0185 (0.0173)\t\n",
      "Pretraining:\tEpoch: [4][60/235]\tLoss 0.0175 (0.0173)\t\n",
      "Pretraining:\tEpoch: [4][70/235]\tLoss 0.0168 (0.0174)\t\n",
      "Pretraining:\tEpoch: [4][80/235]\tLoss 0.0155 (0.0173)\t\n",
      "Pretraining:\tEpoch: [4][90/235]\tLoss 0.0178 (0.0173)\t\n",
      "Pretraining:\tEpoch: [4][100/235]\tLoss 0.0162 (0.0173)\t\n",
      "Pretraining:\tEpoch: [4][110/235]\tLoss 0.0161 (0.0173)\t\n",
      "Pretraining:\tEpoch: [4][120/235]\tLoss 0.0169 (0.0173)\t\n",
      "Pretraining:\tEpoch: [4][130/235]\tLoss 0.0166 (0.0173)\t\n",
      "Pretraining:\tEpoch: [4][140/235]\tLoss 0.0160 (0.0172)\t\n",
      "Pretraining:\tEpoch: [4][150/235]\tLoss 0.0179 (0.0173)\t\n",
      "Pretraining:\tEpoch: [4][160/235]\tLoss 0.0153 (0.0172)\t\n",
      "Pretraining:\tEpoch: [4][170/235]\tLoss 0.0159 (0.0172)\t\n",
      "Pretraining:\tEpoch: [4][180/235]\tLoss 0.0177 (0.0172)\t\n",
      "Pretraining:\tEpoch: [4][190/235]\tLoss 0.0173 (0.0172)\t\n",
      "Pretraining:\tEpoch: [4][200/235]\tLoss 0.0157 (0.0172)\t\n",
      "Pretraining:\tEpoch: [4][210/235]\tLoss 0.0165 (0.0171)\t\n",
      "Pretraining:\tEpoch: [4][220/235]\tLoss 0.0167 (0.0171)\t\n",
      "Pretraining:\tEpoch: [4][230/235]\tLoss 0.0162 (0.0170)\t\n",
      "Pretraining:\t Loss: 0.0170\n",
      "\n",
      "Pretraining:\tEpoch 5/20\n",
      "----------\n",
      "Pretraining:\tEpoch: [5][10/235]\tLoss 0.0153 (0.0161)\t\n",
      "Pretraining:\tEpoch: [5][20/235]\tLoss 0.0164 (0.0162)\t\n",
      "Pretraining:\tEpoch: [5][30/235]\tLoss 0.0157 (0.0161)\t\n",
      "Pretraining:\tEpoch: [5][40/235]\tLoss 0.0163 (0.0163)\t\n",
      "Pretraining:\tEpoch: [5][50/235]\tLoss 0.0173 (0.0162)\t\n",
      "Pretraining:\tEpoch: [5][60/235]\tLoss 0.0163 (0.0162)\t\n",
      "Pretraining:\tEpoch: [5][70/235]\tLoss 0.0158 (0.0162)\t\n",
      "Pretraining:\tEpoch: [5][80/235]\tLoss 0.0146 (0.0162)\t\n",
      "Pretraining:\tEpoch: [5][90/235]\tLoss 0.0163 (0.0162)\t\n",
      "Pretraining:\tEpoch: [5][100/235]\tLoss 0.0154 (0.0162)\t\n",
      "Pretraining:\tEpoch: [5][110/235]\tLoss 0.0152 (0.0162)\t\n",
      "Pretraining:\tEpoch: [5][120/235]\tLoss 0.0159 (0.0162)\t\n",
      "Pretraining:\tEpoch: [5][130/235]\tLoss 0.0156 (0.0162)\t\n",
      "Pretraining:\tEpoch: [5][140/235]\tLoss 0.0151 (0.0161)\t\n",
      "Pretraining:\tEpoch: [5][150/235]\tLoss 0.0170 (0.0162)\t\n",
      "Pretraining:\tEpoch: [5][160/235]\tLoss 0.0145 (0.0161)\t\n",
      "Pretraining:\tEpoch: [5][170/235]\tLoss 0.0150 (0.0161)\t\n",
      "Pretraining:\tEpoch: [5][180/235]\tLoss 0.0167 (0.0161)\t\n",
      "Pretraining:\tEpoch: [5][190/235]\tLoss 0.0164 (0.0161)\t\n",
      "Pretraining:\tEpoch: [5][200/235]\tLoss 0.0148 (0.0161)\t\n",
      "Pretraining:\tEpoch: [5][210/235]\tLoss 0.0157 (0.0161)\t\n",
      "Pretraining:\tEpoch: [5][220/235]\tLoss 0.0157 (0.0161)\t\n",
      "Pretraining:\tEpoch: [5][230/235]\tLoss 0.0154 (0.0160)\t\n",
      "Pretraining:\t Loss: 0.0160\n",
      "\n",
      "Pretraining:\tEpoch 6/20\n",
      "----------\n",
      "Pretraining:\tEpoch: [6][10/235]\tLoss 0.0143 (0.0153)\t\n",
      "Pretraining:\tEpoch: [6][20/235]\tLoss 0.0157 (0.0153)\t\n",
      "Pretraining:\tEpoch: [6][30/235]\tLoss 0.0149 (0.0153)\t\n",
      "Pretraining:\tEpoch: [6][40/235]\tLoss 0.0155 (0.0155)\t\n",
      "Pretraining:\tEpoch: [6][50/235]\tLoss 0.0166 (0.0154)\t\n",
      "Pretraining:\tEpoch: [6][60/235]\tLoss 0.0158 (0.0154)\t\n",
      "Pretraining:\tEpoch: [6][70/235]\tLoss 0.0151 (0.0155)\t\n",
      "Pretraining:\tEpoch: [6][80/235]\tLoss 0.0138 (0.0154)\t\n",
      "Pretraining:\tEpoch: [6][90/235]\tLoss 0.0157 (0.0154)\t\n",
      "Pretraining:\tEpoch: [6][100/235]\tLoss 0.0148 (0.0154)\t\n",
      "Pretraining:\tEpoch: [6][110/235]\tLoss 0.0145 (0.0154)\t\n",
      "Pretraining:\tEpoch: [6][120/235]\tLoss 0.0151 (0.0154)\t\n",
      "Pretraining:\tEpoch: [6][130/235]\tLoss 0.0148 (0.0154)\t\n",
      "Pretraining:\tEpoch: [6][140/235]\tLoss 0.0144 (0.0154)\t\n",
      "Pretraining:\tEpoch: [6][150/235]\tLoss 0.0164 (0.0154)\t\n",
      "Pretraining:\tEpoch: [6][160/235]\tLoss 0.0139 (0.0154)\t\n",
      "Pretraining:\tEpoch: [6][170/235]\tLoss 0.0145 (0.0154)\t\n",
      "Pretraining:\tEpoch: [6][180/235]\tLoss 0.0160 (0.0154)\t\n",
      "Pretraining:\tEpoch: [6][190/235]\tLoss 0.0157 (0.0154)\t\n",
      "Pretraining:\tEpoch: [6][200/235]\tLoss 0.0141 (0.0154)\t\n",
      "Pretraining:\tEpoch: [6][210/235]\tLoss 0.0151 (0.0154)\t\n",
      "Pretraining:\tEpoch: [6][220/235]\tLoss 0.0151 (0.0154)\t\n",
      "Pretraining:\tEpoch: [6][230/235]\tLoss 0.0147 (0.0153)\t\n",
      "Pretraining:\t Loss: 0.0153\n",
      "\n",
      "Pretraining:\tEpoch 7/20\n",
      "----------\n",
      "Pretraining:\tEpoch: [7][10/235]\tLoss 0.0138 (0.0146)\t\n",
      "Pretraining:\tEpoch: [7][20/235]\tLoss 0.0151 (0.0147)\t\n",
      "Pretraining:\tEpoch: [7][30/235]\tLoss 0.0143 (0.0147)\t\n",
      "Pretraining:\tEpoch: [7][40/235]\tLoss 0.0148 (0.0149)\t\n",
      "Pretraining:\tEpoch: [7][50/235]\tLoss 0.0159 (0.0148)\t\n",
      "Pretraining:\tEpoch: [7][60/235]\tLoss 0.0150 (0.0148)\t\n",
      "Pretraining:\tEpoch: [7][70/235]\tLoss 0.0144 (0.0149)\t\n",
      "Pretraining:\tEpoch: [7][80/235]\tLoss 0.0132 (0.0148)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining:\tEpoch: [7][90/235]\tLoss 0.0149 (0.0148)\t\n",
      "Pretraining:\tEpoch: [7][100/235]\tLoss 0.0142 (0.0148)\t\n",
      "Pretraining:\tEpoch: [7][110/235]\tLoss 0.0139 (0.0148)\t\n",
      "Pretraining:\tEpoch: [7][120/235]\tLoss 0.0145 (0.0148)\t\n",
      "Pretraining:\tEpoch: [7][130/235]\tLoss 0.0145 (0.0148)\t\n",
      "Pretraining:\tEpoch: [7][140/235]\tLoss 0.0139 (0.0148)\t\n",
      "Pretraining:\tEpoch: [7][150/235]\tLoss 0.0159 (0.0148)\t\n",
      "Pretraining:\tEpoch: [7][160/235]\tLoss 0.0134 (0.0148)\t\n",
      "Pretraining:\tEpoch: [7][170/235]\tLoss 0.0140 (0.0148)\t\n",
      "Pretraining:\tEpoch: [7][180/235]\tLoss 0.0154 (0.0148)\t\n",
      "Pretraining:\tEpoch: [7][190/235]\tLoss 0.0152 (0.0148)\t\n",
      "Pretraining:\tEpoch: [7][200/235]\tLoss 0.0136 (0.0148)\t\n",
      "Pretraining:\tEpoch: [7][210/235]\tLoss 0.0146 (0.0148)\t\n",
      "Pretraining:\tEpoch: [7][220/235]\tLoss 0.0145 (0.0148)\t\n",
      "Pretraining:\tEpoch: [7][230/235]\tLoss 0.0142 (0.0147)\t\n",
      "Pretraining:\t Loss: 0.0147\n",
      "\n",
      "Pretraining:\tEpoch 8/20\n",
      "----------\n",
      "Pretraining:\tEpoch: [8][10/235]\tLoss 0.0134 (0.0141)\t\n",
      "Pretraining:\tEpoch: [8][20/235]\tLoss 0.0147 (0.0142)\t\n",
      "Pretraining:\tEpoch: [8][30/235]\tLoss 0.0141 (0.0143)\t\n",
      "Pretraining:\tEpoch: [8][40/235]\tLoss 0.0149 (0.0145)\t\n",
      "Pretraining:\tEpoch: [8][50/235]\tLoss 0.0155 (0.0145)\t\n",
      "Pretraining:\tEpoch: [8][60/235]\tLoss 0.0146 (0.0145)\t\n",
      "Pretraining:\tEpoch: [8][70/235]\tLoss 0.0140 (0.0146)\t\n",
      "Pretraining:\tEpoch: [8][80/235]\tLoss 0.0129 (0.0145)\t\n",
      "Pretraining:\tEpoch: [8][90/235]\tLoss 0.0145 (0.0145)\t\n",
      "Pretraining:\tEpoch: [8][100/235]\tLoss 0.0138 (0.0145)\t\n",
      "Pretraining:\tEpoch: [8][110/235]\tLoss 0.0135 (0.0145)\t\n",
      "Pretraining:\tEpoch: [8][120/235]\tLoss 0.0140 (0.0145)\t\n",
      "Pretraining:\tEpoch: [8][130/235]\tLoss 0.0139 (0.0145)\t\n",
      "Pretraining:\tEpoch: [8][140/235]\tLoss 0.0134 (0.0144)\t\n",
      "Pretraining:\tEpoch: [8][150/235]\tLoss 0.0154 (0.0145)\t\n",
      "Pretraining:\tEpoch: [8][160/235]\tLoss 0.0130 (0.0144)\t\n",
      "Pretraining:\tEpoch: [8][170/235]\tLoss 0.0135 (0.0144)\t\n",
      "Pretraining:\tEpoch: [8][180/235]\tLoss 0.0149 (0.0144)\t\n",
      "Pretraining:\tEpoch: [8][190/235]\tLoss 0.0148 (0.0144)\t\n",
      "Pretraining:\tEpoch: [8][200/235]\tLoss 0.0131 (0.0144)\t\n",
      "Pretraining:\tEpoch: [8][210/235]\tLoss 0.0142 (0.0144)\t\n",
      "Pretraining:\tEpoch: [8][220/235]\tLoss 0.0142 (0.0144)\t\n",
      "Pretraining:\tEpoch: [8][230/235]\tLoss 0.0139 (0.0144)\t\n",
      "Pretraining:\t Loss: 0.0143\n",
      "\n",
      "Pretraining:\tEpoch 9/20\n",
      "----------\n",
      "Pretraining:\tEpoch: [9][10/235]\tLoss 0.0130 (0.0137)\t\n",
      "Pretraining:\tEpoch: [9][20/235]\tLoss 0.0144 (0.0138)\t\n",
      "Pretraining:\tEpoch: [9][30/235]\tLoss 0.0137 (0.0139)\t\n",
      "Pretraining:\tEpoch: [9][40/235]\tLoss 0.0139 (0.0140)\t\n",
      "Pretraining:\tEpoch: [9][50/235]\tLoss 0.0150 (0.0139)\t\n",
      "Pretraining:\tEpoch: [9][60/235]\tLoss 0.0141 (0.0140)\t\n",
      "Pretraining:\tEpoch: [9][70/235]\tLoss 0.0135 (0.0140)\t\n",
      "Pretraining:\tEpoch: [9][80/235]\tLoss 0.0124 (0.0140)\t\n",
      "Pretraining:\tEpoch: [9][90/235]\tLoss 0.0141 (0.0140)\t\n",
      "Pretraining:\tEpoch: [9][100/235]\tLoss 0.0133 (0.0139)\t\n",
      "Pretraining:\tEpoch: [9][110/235]\tLoss 0.0131 (0.0139)\t\n",
      "Pretraining:\tEpoch: [9][120/235]\tLoss 0.0136 (0.0139)\t\n",
      "Pretraining:\tEpoch: [9][130/235]\tLoss 0.0135 (0.0139)\t\n",
      "Pretraining:\tEpoch: [9][140/235]\tLoss 0.0131 (0.0139)\t\n",
      "Pretraining:\tEpoch: [9][150/235]\tLoss 0.0151 (0.0140)\t\n",
      "Pretraining:\tEpoch: [9][160/235]\tLoss 0.0127 (0.0139)\t\n",
      "Pretraining:\tEpoch: [9][170/235]\tLoss 0.0132 (0.0139)\t\n",
      "Pretraining:\tEpoch: [9][180/235]\tLoss 0.0146 (0.0139)\t\n",
      "Pretraining:\tEpoch: [9][190/235]\tLoss 0.0144 (0.0140)\t\n",
      "Pretraining:\tEpoch: [9][200/235]\tLoss 0.0127 (0.0140)\t\n",
      "Pretraining:\tEpoch: [9][210/235]\tLoss 0.0138 (0.0139)\t\n",
      "Pretraining:\tEpoch: [9][220/235]\tLoss 0.0138 (0.0139)\t\n",
      "Pretraining:\tEpoch: [9][230/235]\tLoss 0.0135 (0.0139)\t\n",
      "Pretraining:\t Loss: 0.0139\n",
      "\n",
      "Pretraining:\tEpoch 10/20\n",
      "----------\n",
      "Pretraining:\tEpoch: [10][10/235]\tLoss 0.0127 (0.0133)\t\n",
      "Pretraining:\tEpoch: [10][20/235]\tLoss 0.0139 (0.0134)\t\n",
      "Pretraining:\tEpoch: [10][30/235]\tLoss 0.0132 (0.0135)\t\n",
      "Pretraining:\tEpoch: [10][40/235]\tLoss 0.0136 (0.0136)\t\n",
      "Pretraining:\tEpoch: [10][50/235]\tLoss 0.0146 (0.0135)\t\n",
      "Pretraining:\tEpoch: [10][60/235]\tLoss 0.0137 (0.0136)\t\n",
      "Pretraining:\tEpoch: [10][70/235]\tLoss 0.0132 (0.0136)\t\n",
      "Pretraining:\tEpoch: [10][80/235]\tLoss 0.0121 (0.0136)\t\n",
      "Pretraining:\tEpoch: [10][90/235]\tLoss 0.0137 (0.0136)\t\n",
      "Pretraining:\tEpoch: [10][100/235]\tLoss 0.0130 (0.0135)\t\n",
      "Pretraining:\tEpoch: [10][110/235]\tLoss 0.0127 (0.0136)\t\n",
      "Pretraining:\tEpoch: [10][120/235]\tLoss 0.0133 (0.0136)\t\n",
      "Pretraining:\tEpoch: [10][130/235]\tLoss 0.0132 (0.0136)\t\n",
      "Pretraining:\tEpoch: [10][140/235]\tLoss 0.0127 (0.0135)\t\n",
      "Pretraining:\tEpoch: [10][150/235]\tLoss 0.0148 (0.0136)\t\n",
      "Pretraining:\tEpoch: [10][160/235]\tLoss 0.0124 (0.0136)\t\n",
      "Pretraining:\tEpoch: [10][170/235]\tLoss 0.0128 (0.0136)\t\n",
      "Pretraining:\tEpoch: [10][180/235]\tLoss 0.0143 (0.0136)\t\n",
      "Pretraining:\tEpoch: [10][190/235]\tLoss 0.0140 (0.0136)\t\n",
      "Pretraining:\tEpoch: [10][200/235]\tLoss 0.0124 (0.0136)\t\n",
      "Pretraining:\tEpoch: [10][210/235]\tLoss 0.0135 (0.0136)\t\n",
      "Pretraining:\tEpoch: [10][220/235]\tLoss 0.0135 (0.0136)\t\n",
      "Pretraining:\tEpoch: [10][230/235]\tLoss 0.0132 (0.0136)\t\n",
      "Pretraining:\t Loss: 0.0135\n",
      "\n",
      "Pretraining:\tEpoch 11/20\n",
      "----------\n",
      "Pretraining:\tEpoch: [11][10/235]\tLoss 0.0125 (0.0130)\t\n",
      "Pretraining:\tEpoch: [11][20/235]\tLoss 0.0135 (0.0131)\t\n",
      "Pretraining:\tEpoch: [11][30/235]\tLoss 0.0128 (0.0131)\t\n",
      "Pretraining:\tEpoch: [11][40/235]\tLoss 0.0133 (0.0133)\t\n",
      "Pretraining:\tEpoch: [11][50/235]\tLoss 0.0143 (0.0132)\t\n",
      "Pretraining:\tEpoch: [11][60/235]\tLoss 0.0134 (0.0133)\t\n",
      "Pretraining:\tEpoch: [11][70/235]\tLoss 0.0129 (0.0133)\t\n",
      "Pretraining:\tEpoch: [11][80/235]\tLoss 0.0119 (0.0132)\t\n",
      "Pretraining:\tEpoch: [11][90/235]\tLoss 0.0134 (0.0133)\t\n",
      "Pretraining:\tEpoch: [11][100/235]\tLoss 0.0127 (0.0132)\t\n",
      "Pretraining:\tEpoch: [11][110/235]\tLoss 0.0124 (0.0132)\t\n",
      "Pretraining:\tEpoch: [11][120/235]\tLoss 0.0131 (0.0133)\t\n",
      "Pretraining:\tEpoch: [11][130/235]\tLoss 0.0130 (0.0133)\t\n",
      "Pretraining:\tEpoch: [11][140/235]\tLoss 0.0125 (0.0132)\t\n",
      "Pretraining:\tEpoch: [11][150/235]\tLoss 0.0145 (0.0133)\t\n",
      "Pretraining:\tEpoch: [11][160/235]\tLoss 0.0121 (0.0133)\t\n",
      "Pretraining:\tEpoch: [11][170/235]\tLoss 0.0125 (0.0133)\t\n",
      "Pretraining:\tEpoch: [11][180/235]\tLoss 0.0140 (0.0133)\t\n",
      "Pretraining:\tEpoch: [11][190/235]\tLoss 0.0138 (0.0133)\t\n",
      "Pretraining:\tEpoch: [11][200/235]\tLoss 0.0121 (0.0133)\t\n",
      "Pretraining:\tEpoch: [11][210/235]\tLoss 0.0132 (0.0133)\t\n",
      "Pretraining:\tEpoch: [11][220/235]\tLoss 0.0132 (0.0133)\t\n",
      "Pretraining:\tEpoch: [11][230/235]\tLoss 0.0129 (0.0133)\t\n",
      "Pretraining:\t Loss: 0.0132\n",
      "\n",
      "Pretraining:\tEpoch 12/20\n",
      "----------\n",
      "Pretraining:\tEpoch: [12][10/235]\tLoss 0.0122 (0.0127)\t\n",
      "Pretraining:\tEpoch: [12][20/235]\tLoss 0.0132 (0.0128)\t\n",
      "Pretraining:\tEpoch: [12][30/235]\tLoss 0.0125 (0.0129)\t\n",
      "Pretraining:\tEpoch: [12][40/235]\tLoss 0.0130 (0.0130)\t\n",
      "Pretraining:\tEpoch: [12][50/235]\tLoss 0.0140 (0.0129)\t\n",
      "Pretraining:\tEpoch: [12][60/235]\tLoss 0.0133 (0.0130)\t\n",
      "Pretraining:\tEpoch: [12][70/235]\tLoss 0.0127 (0.0130)\t\n",
      "Pretraining:\tEpoch: [12][80/235]\tLoss 0.0117 (0.0130)\t\n",
      "Pretraining:\tEpoch: [12][90/235]\tLoss 0.0133 (0.0130)\t\n",
      "Pretraining:\tEpoch: [12][100/235]\tLoss 0.0125 (0.0130)\t\n",
      "Pretraining:\tEpoch: [12][110/235]\tLoss 0.0121 (0.0130)\t\n",
      "Pretraining:\tEpoch: [12][120/235]\tLoss 0.0128 (0.0130)\t\n",
      "Pretraining:\tEpoch: [12][130/235]\tLoss 0.0127 (0.0130)\t\n",
      "Pretraining:\tEpoch: [12][140/235]\tLoss 0.0122 (0.0130)\t\n",
      "Pretraining:\tEpoch: [12][150/235]\tLoss 0.0142 (0.0130)\t\n",
      "Pretraining:\tEpoch: [12][160/235]\tLoss 0.0119 (0.0130)\t\n",
      "Pretraining:\tEpoch: [12][170/235]\tLoss 0.0123 (0.0130)\t\n",
      "Pretraining:\tEpoch: [12][180/235]\tLoss 0.0137 (0.0130)\t\n",
      "Pretraining:\tEpoch: [12][190/235]\tLoss 0.0136 (0.0131)\t\n",
      "Pretraining:\tEpoch: [12][200/235]\tLoss 0.0119 (0.0131)\t\n",
      "Pretraining:\tEpoch: [12][210/235]\tLoss 0.0130 (0.0131)\t\n",
      "Pretraining:\tEpoch: [12][220/235]\tLoss 0.0130 (0.0130)\t\n",
      "Pretraining:\tEpoch: [12][230/235]\tLoss 0.0127 (0.0130)\t\n",
      "Pretraining:\t Loss: 0.0130\n",
      "\n",
      "Pretraining:\tEpoch 13/20\n",
      "----------\n",
      "Pretraining:\tEpoch: [13][10/235]\tLoss 0.0119 (0.0125)\t\n",
      "Pretraining:\tEpoch: [13][20/235]\tLoss 0.0129 (0.0126)\t\n",
      "Pretraining:\tEpoch: [13][30/235]\tLoss 0.0123 (0.0126)\t\n",
      "Pretraining:\tEpoch: [13][40/235]\tLoss 0.0128 (0.0127)\t\n",
      "Pretraining:\tEpoch: [13][50/235]\tLoss 0.0138 (0.0127)\t\n",
      "Pretraining:\tEpoch: [13][60/235]\tLoss 0.0130 (0.0127)\t\n",
      "Pretraining:\tEpoch: [13][70/235]\tLoss 0.0124 (0.0128)\t\n",
      "Pretraining:\tEpoch: [13][80/235]\tLoss 0.0115 (0.0127)\t\n",
      "Pretraining:\tEpoch: [13][90/235]\tLoss 0.0130 (0.0128)\t\n",
      "Pretraining:\tEpoch: [13][100/235]\tLoss 0.0121 (0.0127)\t\n",
      "Pretraining:\tEpoch: [13][110/235]\tLoss 0.0119 (0.0128)\t\n",
      "Pretraining:\tEpoch: [13][120/235]\tLoss 0.0125 (0.0128)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining:\tEpoch: [13][130/235]\tLoss 0.0125 (0.0128)\t\n",
      "Pretraining:\tEpoch: [13][140/235]\tLoss 0.0120 (0.0127)\t\n",
      "Pretraining:\tEpoch: [13][150/235]\tLoss 0.0140 (0.0128)\t\n",
      "Pretraining:\tEpoch: [13][160/235]\tLoss 0.0117 (0.0128)\t\n",
      "Pretraining:\tEpoch: [13][170/235]\tLoss 0.0121 (0.0128)\t\n",
      "Pretraining:\tEpoch: [13][180/235]\tLoss 0.0135 (0.0128)\t\n",
      "Pretraining:\tEpoch: [13][190/235]\tLoss 0.0134 (0.0128)\t\n",
      "Pretraining:\tEpoch: [13][200/235]\tLoss 0.0117 (0.0128)\t\n",
      "Pretraining:\tEpoch: [13][210/235]\tLoss 0.0128 (0.0128)\t\n",
      "Pretraining:\tEpoch: [13][220/235]\tLoss 0.0127 (0.0128)\t\n",
      "Pretraining:\tEpoch: [13][230/235]\tLoss 0.0124 (0.0128)\t\n",
      "Pretraining:\t Loss: 0.0128\n",
      "\n",
      "Pretraining:\tEpoch 14/20\n",
      "----------\n",
      "Pretraining:\tEpoch: [14][10/235]\tLoss 0.0118 (0.0123)\t\n",
      "Pretraining:\tEpoch: [14][20/235]\tLoss 0.0127 (0.0124)\t\n",
      "Pretraining:\tEpoch: [14][30/235]\tLoss 0.0120 (0.0124)\t\n",
      "Pretraining:\tEpoch: [14][40/235]\tLoss 0.0126 (0.0125)\t\n",
      "Pretraining:\tEpoch: [14][50/235]\tLoss 0.0136 (0.0125)\t\n",
      "Pretraining:\tEpoch: [14][60/235]\tLoss 0.0129 (0.0125)\t\n",
      "Pretraining:\tEpoch: [14][70/235]\tLoss 0.0122 (0.0126)\t\n",
      "Pretraining:\tEpoch: [14][80/235]\tLoss 0.0113 (0.0125)\t\n",
      "Pretraining:\tEpoch: [14][90/235]\tLoss 0.0128 (0.0126)\t\n",
      "Pretraining:\tEpoch: [14][100/235]\tLoss 0.0120 (0.0125)\t\n",
      "Pretraining:\tEpoch: [14][110/235]\tLoss 0.0117 (0.0126)\t\n",
      "Pretraining:\tEpoch: [14][120/235]\tLoss 0.0123 (0.0126)\t\n",
      "Pretraining:\tEpoch: [14][130/235]\tLoss 0.0124 (0.0126)\t\n",
      "Pretraining:\tEpoch: [14][140/235]\tLoss 0.0119 (0.0126)\t\n",
      "Pretraining:\tEpoch: [14][150/235]\tLoss 0.0138 (0.0126)\t\n",
      "Pretraining:\tEpoch: [14][160/235]\tLoss 0.0115 (0.0126)\t\n",
      "Pretraining:\tEpoch: [14][170/235]\tLoss 0.0119 (0.0126)\t\n",
      "Pretraining:\tEpoch: [14][180/235]\tLoss 0.0133 (0.0126)\t\n",
      "Pretraining:\tEpoch: [14][190/235]\tLoss 0.0132 (0.0126)\t\n",
      "Pretraining:\tEpoch: [14][200/235]\tLoss 0.0116 (0.0126)\t\n",
      "Pretraining:\tEpoch: [14][210/235]\tLoss 0.0126 (0.0126)\t\n",
      "Pretraining:\tEpoch: [14][220/235]\tLoss 0.0125 (0.0126)\t\n",
      "Pretraining:\tEpoch: [14][230/235]\tLoss 0.0124 (0.0126)\t\n",
      "Pretraining:\t Loss: 0.0126\n",
      "\n",
      "Pretraining:\tEpoch 15/20\n",
      "----------\n",
      "Pretraining:\tEpoch: [15][10/235]\tLoss 0.0115 (0.0121)\t\n",
      "Pretraining:\tEpoch: [15][20/235]\tLoss 0.0126 (0.0122)\t\n",
      "Pretraining:\tEpoch: [15][30/235]\tLoss 0.0118 (0.0122)\t\n",
      "Pretraining:\tEpoch: [15][40/235]\tLoss 0.0123 (0.0123)\t\n",
      "Pretraining:\tEpoch: [15][50/235]\tLoss 0.0134 (0.0123)\t\n",
      "Pretraining:\tEpoch: [15][60/235]\tLoss 0.0127 (0.0124)\t\n",
      "Pretraining:\tEpoch: [15][70/235]\tLoss 0.0121 (0.0124)\t\n",
      "Pretraining:\tEpoch: [15][80/235]\tLoss 0.0111 (0.0124)\t\n",
      "Pretraining:\tEpoch: [15][90/235]\tLoss 0.0130 (0.0124)\t\n",
      "Pretraining:\tEpoch: [15][100/235]\tLoss 0.0119 (0.0124)\t\n",
      "Pretraining:\tEpoch: [15][110/235]\tLoss 0.0116 (0.0124)\t\n",
      "Pretraining:\tEpoch: [15][120/235]\tLoss 0.0122 (0.0124)\t\n",
      "Pretraining:\tEpoch: [15][130/235]\tLoss 0.0122 (0.0125)\t\n",
      "Pretraining:\tEpoch: [15][140/235]\tLoss 0.0118 (0.0124)\t\n",
      "Pretraining:\tEpoch: [15][150/235]\tLoss 0.0137 (0.0125)\t\n",
      "Pretraining:\tEpoch: [15][160/235]\tLoss 0.0113 (0.0124)\t\n",
      "Pretraining:\tEpoch: [15][170/235]\tLoss 0.0117 (0.0124)\t\n",
      "Pretraining:\tEpoch: [15][180/235]\tLoss 0.0131 (0.0124)\t\n",
      "Pretraining:\tEpoch: [15][190/235]\tLoss 0.0131 (0.0125)\t\n",
      "Pretraining:\tEpoch: [15][200/235]\tLoss 0.0114 (0.0125)\t\n",
      "Pretraining:\tEpoch: [15][210/235]\tLoss 0.0124 (0.0125)\t\n",
      "Pretraining:\tEpoch: [15][220/235]\tLoss 0.0123 (0.0125)\t\n",
      "Pretraining:\tEpoch: [15][230/235]\tLoss 0.0121 (0.0125)\t\n",
      "Pretraining:\t Loss: 0.0124\n",
      "\n",
      "Pretraining:\tEpoch 16/20\n",
      "----------\n",
      "Pretraining:\tEpoch: [16][10/235]\tLoss 0.0114 (0.0120)\t\n",
      "Pretraining:\tEpoch: [16][20/235]\tLoss 0.0124 (0.0121)\t\n",
      "Pretraining:\tEpoch: [16][30/235]\tLoss 0.0116 (0.0121)\t\n",
      "Pretraining:\tEpoch: [16][40/235]\tLoss 0.0122 (0.0122)\t\n",
      "Pretraining:\tEpoch: [16][50/235]\tLoss 0.0132 (0.0121)\t\n",
      "Pretraining:\tEpoch: [16][60/235]\tLoss 0.0125 (0.0122)\t\n",
      "Pretraining:\tEpoch: [16][70/235]\tLoss 0.0119 (0.0122)\t\n",
      "Pretraining:\tEpoch: [16][80/235]\tLoss 0.0111 (0.0122)\t\n",
      "Pretraining:\tEpoch: [16][90/235]\tLoss 0.0126 (0.0122)\t\n",
      "Pretraining:\tEpoch: [16][100/235]\tLoss 0.0117 (0.0122)\t\n",
      "Pretraining:\tEpoch: [16][110/235]\tLoss 0.0115 (0.0122)\t\n",
      "Pretraining:\tEpoch: [16][120/235]\tLoss 0.0121 (0.0123)\t\n",
      "Pretraining:\tEpoch: [16][130/235]\tLoss 0.0120 (0.0123)\t\n",
      "Pretraining:\tEpoch: [16][140/235]\tLoss 0.0115 (0.0123)\t\n",
      "Pretraining:\tEpoch: [16][150/235]\tLoss 0.0135 (0.0123)\t\n",
      "Pretraining:\tEpoch: [16][160/235]\tLoss 0.0112 (0.0123)\t\n",
      "Pretraining:\tEpoch: [16][170/235]\tLoss 0.0116 (0.0123)\t\n",
      "Pretraining:\tEpoch: [16][180/235]\tLoss 0.0129 (0.0123)\t\n",
      "Pretraining:\tEpoch: [16][190/235]\tLoss 0.0130 (0.0123)\t\n",
      "Pretraining:\tEpoch: [16][200/235]\tLoss 0.0112 (0.0123)\t\n",
      "Pretraining:\tEpoch: [16][210/235]\tLoss 0.0123 (0.0123)\t\n",
      "Pretraining:\tEpoch: [16][220/235]\tLoss 0.0122 (0.0123)\t\n",
      "Pretraining:\tEpoch: [16][230/235]\tLoss 0.0120 (0.0123)\t\n",
      "Pretraining:\t Loss: 0.0123\n",
      "\n",
      "Pretraining:\tEpoch 17/20\n",
      "----------\n",
      "Pretraining:\tEpoch: [17][10/235]\tLoss 0.0112 (0.0119)\t\n",
      "Pretraining:\tEpoch: [17][20/235]\tLoss 0.0123 (0.0120)\t\n",
      "Pretraining:\tEpoch: [17][30/235]\tLoss 0.0115 (0.0120)\t\n",
      "Pretraining:\tEpoch: [17][40/235]\tLoss 0.0121 (0.0121)\t\n",
      "Pretraining:\tEpoch: [17][50/235]\tLoss 0.0130 (0.0121)\t\n",
      "Pretraining:\tEpoch: [17][60/235]\tLoss 0.0123 (0.0121)\t\n",
      "Pretraining:\tEpoch: [17][70/235]\tLoss 0.0120 (0.0122)\t\n",
      "Pretraining:\tEpoch: [17][80/235]\tLoss 0.0110 (0.0121)\t\n",
      "Pretraining:\tEpoch: [17][90/235]\tLoss 0.0128 (0.0122)\t\n",
      "Pretraining:\tEpoch: [17][100/235]\tLoss 0.0115 (0.0122)\t\n",
      "Pretraining:\tEpoch: [17][110/235]\tLoss 0.0114 (0.0122)\t\n",
      "Pretraining:\tEpoch: [17][120/235]\tLoss 0.0118 (0.0122)\t\n",
      "Pretraining:\tEpoch: [17][130/235]\tLoss 0.0119 (0.0122)\t\n",
      "Pretraining:\tEpoch: [17][140/235]\tLoss 0.0113 (0.0122)\t\n",
      "Pretraining:\tEpoch: [17][150/235]\tLoss 0.0133 (0.0122)\t\n",
      "Pretraining:\tEpoch: [17][160/235]\tLoss 0.0110 (0.0122)\t\n",
      "Pretraining:\tEpoch: [17][170/235]\tLoss 0.0114 (0.0122)\t\n",
      "Pretraining:\tEpoch: [17][180/235]\tLoss 0.0126 (0.0121)\t\n",
      "Pretraining:\tEpoch: [17][190/235]\tLoss 0.0125 (0.0122)\t\n",
      "Pretraining:\tEpoch: [17][200/235]\tLoss 0.0111 (0.0122)\t\n",
      "Pretraining:\tEpoch: [17][210/235]\tLoss 0.0123 (0.0122)\t\n",
      "Pretraining:\tEpoch: [17][220/235]\tLoss 0.0121 (0.0122)\t\n",
      "Pretraining:\tEpoch: [17][230/235]\tLoss 0.0119 (0.0121)\t\n",
      "Pretraining:\t Loss: 0.0121\n",
      "\n",
      "Pretraining:\tEpoch 18/20\n",
      "----------\n",
      "Pretraining:\tEpoch: [18][10/235]\tLoss 0.0111 (0.0117)\t\n",
      "Pretraining:\tEpoch: [18][20/235]\tLoss 0.0120 (0.0118)\t\n",
      "Pretraining:\tEpoch: [18][30/235]\tLoss 0.0115 (0.0118)\t\n",
      "Pretraining:\tEpoch: [18][40/235]\tLoss 0.0120 (0.0119)\t\n",
      "Pretraining:\tEpoch: [18][50/235]\tLoss 0.0129 (0.0118)\t\n",
      "Pretraining:\tEpoch: [18][60/235]\tLoss 0.0120 (0.0119)\t\n",
      "Pretraining:\tEpoch: [18][70/235]\tLoss 0.0117 (0.0119)\t\n",
      "Pretraining:\tEpoch: [18][80/235]\tLoss 0.0107 (0.0119)\t\n",
      "Pretraining:\tEpoch: [18][90/235]\tLoss 0.0124 (0.0119)\t\n",
      "Pretraining:\tEpoch: [18][100/235]\tLoss 0.0113 (0.0119)\t\n",
      "Pretraining:\tEpoch: [18][110/235]\tLoss 0.0110 (0.0119)\t\n",
      "Pretraining:\tEpoch: [18][120/235]\tLoss 0.0117 (0.0119)\t\n",
      "Pretraining:\tEpoch: [18][130/235]\tLoss 0.0116 (0.0119)\t\n",
      "Pretraining:\tEpoch: [18][140/235]\tLoss 0.0112 (0.0119)\t\n",
      "Pretraining:\tEpoch: [18][150/235]\tLoss 0.0131 (0.0119)\t\n",
      "Pretraining:\tEpoch: [18][160/235]\tLoss 0.0108 (0.0119)\t\n",
      "Pretraining:\tEpoch: [18][170/235]\tLoss 0.0112 (0.0119)\t\n",
      "Pretraining:\tEpoch: [18][180/235]\tLoss 0.0125 (0.0119)\t\n",
      "Pretraining:\tEpoch: [18][190/235]\tLoss 0.0123 (0.0119)\t\n",
      "Pretraining:\tEpoch: [18][200/235]\tLoss 0.0110 (0.0120)\t\n",
      "Pretraining:\tEpoch: [18][210/235]\tLoss 0.0120 (0.0119)\t\n",
      "Pretraining:\tEpoch: [18][220/235]\tLoss 0.0120 (0.0119)\t\n",
      "Pretraining:\tEpoch: [18][230/235]\tLoss 0.0117 (0.0119)\t\n",
      "Pretraining:\t Loss: 0.0119\n",
      "\n",
      "Pretraining:\tEpoch 19/20\n",
      "----------\n",
      "Pretraining:\tEpoch: [19][10/235]\tLoss 0.0109 (0.0115)\t\n",
      "Pretraining:\tEpoch: [19][20/235]\tLoss 0.0118 (0.0116)\t\n",
      "Pretraining:\tEpoch: [19][30/235]\tLoss 0.0112 (0.0116)\t\n",
      "Pretraining:\tEpoch: [19][40/235]\tLoss 0.0117 (0.0117)\t\n",
      "Pretraining:\tEpoch: [19][50/235]\tLoss 0.0126 (0.0116)\t\n",
      "Pretraining:\tEpoch: [19][60/235]\tLoss 0.0119 (0.0117)\t\n",
      "Pretraining:\tEpoch: [19][70/235]\tLoss 0.0115 (0.0117)\t\n",
      "Pretraining:\tEpoch: [19][80/235]\tLoss 0.0105 (0.0117)\t\n",
      "Pretraining:\tEpoch: [19][90/235]\tLoss 0.0120 (0.0117)\t\n",
      "Pretraining:\tEpoch: [19][100/235]\tLoss 0.0111 (0.0117)\t\n",
      "Pretraining:\tEpoch: [19][110/235]\tLoss 0.0108 (0.0117)\t\n",
      "Pretraining:\tEpoch: [19][120/235]\tLoss 0.0115 (0.0117)\t\n",
      "Pretraining:\tEpoch: [19][130/235]\tLoss 0.0114 (0.0117)\t\n",
      "Pretraining:\tEpoch: [19][140/235]\tLoss 0.0110 (0.0117)\t\n",
      "Pretraining:\tEpoch: [19][150/235]\tLoss 0.0130 (0.0117)\t\n",
      "Pretraining:\tEpoch: [19][160/235]\tLoss 0.0107 (0.0117)\t\n",
      "Pretraining:\tEpoch: [19][170/235]\tLoss 0.0111 (0.0117)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining:\tEpoch: [19][180/235]\tLoss 0.0124 (0.0117)\t\n",
      "Pretraining:\tEpoch: [19][190/235]\tLoss 0.0122 (0.0117)\t\n",
      "Pretraining:\tEpoch: [19][200/235]\tLoss 0.0108 (0.0118)\t\n",
      "Pretraining:\tEpoch: [19][210/235]\tLoss 0.0118 (0.0118)\t\n",
      "Pretraining:\tEpoch: [19][220/235]\tLoss 0.0118 (0.0117)\t\n",
      "Pretraining:\tEpoch: [19][230/235]\tLoss 0.0115 (0.0117)\t\n",
      "Pretraining:\t Loss: 0.0117\n",
      "\n",
      "Pretraining:\tEpoch 20/20\n",
      "----------\n",
      "Pretraining:\tEpoch: [20][10/235]\tLoss 0.0107 (0.0113)\t\n",
      "Pretraining:\tEpoch: [20][20/235]\tLoss 0.0116 (0.0114)\t\n",
      "Pretraining:\tEpoch: [20][30/235]\tLoss 0.0109 (0.0114)\t\n",
      "Pretraining:\tEpoch: [20][40/235]\tLoss 0.0116 (0.0115)\t\n",
      "Pretraining:\tEpoch: [20][50/235]\tLoss 0.0124 (0.0114)\t\n",
      "Pretraining:\tEpoch: [20][60/235]\tLoss 0.0117 (0.0115)\t\n",
      "Pretraining:\tEpoch: [20][70/235]\tLoss 0.0113 (0.0115)\t\n",
      "Pretraining:\tEpoch: [20][80/235]\tLoss 0.0103 (0.0115)\t\n",
      "Pretraining:\tEpoch: [20][90/235]\tLoss 0.0118 (0.0115)\t\n",
      "Pretraining:\tEpoch: [20][100/235]\tLoss 0.0110 (0.0115)\t\n",
      "Pretraining:\tEpoch: [20][110/235]\tLoss 0.0106 (0.0115)\t\n",
      "Pretraining:\tEpoch: [20][120/235]\tLoss 0.0113 (0.0115)\t\n",
      "Pretraining:\tEpoch: [20][130/235]\tLoss 0.0113 (0.0115)\t\n",
      "Pretraining:\tEpoch: [20][140/235]\tLoss 0.0108 (0.0115)\t\n",
      "Pretraining:\tEpoch: [20][150/235]\tLoss 0.0128 (0.0115)\t\n",
      "Pretraining:\tEpoch: [20][160/235]\tLoss 0.0106 (0.0115)\t\n",
      "Pretraining:\tEpoch: [20][170/235]\tLoss 0.0110 (0.0115)\t\n",
      "Pretraining:\tEpoch: [20][180/235]\tLoss 0.0122 (0.0115)\t\n",
      "Pretraining:\tEpoch: [20][190/235]\tLoss 0.0120 (0.0116)\t\n",
      "Pretraining:\tEpoch: [20][200/235]\tLoss 0.0107 (0.0116)\t\n",
      "Pretraining:\tEpoch: [20][210/235]\tLoss 0.0116 (0.0116)\t\n",
      "Pretraining:\tEpoch: [20][220/235]\tLoss 0.0115 (0.0116)\t\n",
      "Pretraining:\tEpoch: [20][230/235]\tLoss 0.0113 (0.0116)\t\n",
      "Pretraining:\t Loss: 0.0116\n",
      "\n",
      "Pretraining complete in 0m 50s\n",
      "\n",
      "Initializing cluster centers based on K-means\n",
      "\n",
      "Begin clusters training\n",
      "\n",
      "Updating target distribution\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\linear_assignment_.py:18: FutureWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\linear_assignment_.py:124: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMI: 0.74255\tARI: 0.68582\tAcc 0.80833\n",
      "\n",
      "Epoch 1/1000\n",
      "----------\n",
      "Epoch: [1][10/235]\tLoss 0.0266 (0.0303)\tLoss_recovery 0.0139 (0.0150)\tLoss clustering 0.0127 (0.0152)\t\n",
      "Epoch: [1][20/235]\tLoss 0.0257 (0.0285)\tLoss_recovery 0.0138 (0.0145)\tLoss clustering 0.0119 (0.0139)\t\n",
      "Epoch: [1][30/235]\tLoss 0.0242 (0.0271)\tLoss_recovery 0.0128 (0.0141)\tLoss clustering 0.0114 (0.0130)\t\n",
      "Epoch: [1][40/235]\tLoss 0.0225 (0.0262)\tLoss_recovery 0.0134 (0.0140)\tLoss clustering 0.0091 (0.0122)\t\n",
      "Epoch: [1][50/235]\tLoss 0.0230 (0.0255)\tLoss_recovery 0.0142 (0.0138)\tLoss clustering 0.0089 (0.0116)\t\n",
      "Epoch: [1][60/235]\tLoss 0.0229 (0.0249)\tLoss_recovery 0.0135 (0.0138)\tLoss clustering 0.0094 (0.0111)\t\n",
      "Epoch: [1][70/235]\tLoss 0.0207 (0.0244)\tLoss_recovery 0.0128 (0.0137)\tLoss clustering 0.0080 (0.0107)\t\n",
      "Epoch: [1][80/235]\tLoss 0.0204 (0.0240)\tLoss_recovery 0.0122 (0.0136)\tLoss clustering 0.0083 (0.0104)\t\n",
      "\n",
      "Updating target distribution:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\linear_assignment_.py:124: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMI: 0.76430\tARI: 0.70587\tAcc 0.81810\t\n",
      "Epoch: [1][90/235]\tLoss 0.0397 (0.0257)\tLoss_recovery 0.0145 (0.0137)\tLoss clustering 0.0252 (0.0121)\t\n",
      "Epoch: [1][100/235]\tLoss 0.0378 (0.0269)\tLoss_recovery 0.0146 (0.0138)\tLoss clustering 0.0232 (0.0131)\t\n",
      "Epoch: [1][110/235]\tLoss 0.0383 (0.0279)\tLoss_recovery 0.0145 (0.0140)\tLoss clustering 0.0239 (0.0139)\t\n",
      "Epoch: [1][120/235]\tLoss 0.0339 (0.0285)\tLoss_recovery 0.0154 (0.0141)\tLoss clustering 0.0185 (0.0144)\t\n",
      "Epoch: [1][130/235]\tLoss 0.0345 (0.0290)\tLoss_recovery 0.0154 (0.0142)\tLoss clustering 0.0191 (0.0148)\t\n",
      "Epoch: [1][140/235]\tLoss 0.0341 (0.0294)\tLoss_recovery 0.0149 (0.0143)\tLoss clustering 0.0192 (0.0151)\t\n",
      "Epoch: [1][150/235]\tLoss 0.0360 (0.0299)\tLoss_recovery 0.0171 (0.0144)\tLoss clustering 0.0189 (0.0154)\t\n",
      "Epoch: [1][160/235]\tLoss 0.0348 (0.0302)\tLoss_recovery 0.0148 (0.0145)\tLoss clustering 0.0201 (0.0156)\t\n",
      "\n",
      "Updating target distribution:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\linear_assignment_.py:124: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMI: 0.78369\tARI: 0.72530\tAcc 0.82678\t\n",
      "Epoch: [1][170/235]\tLoss 0.0452 (0.0310)\tLoss_recovery 0.0160 (0.0146)\tLoss clustering 0.0292 (0.0164)\t\n",
      "Epoch: [1][180/235]\tLoss 0.0430 (0.0317)\tLoss_recovery 0.0170 (0.0147)\tLoss clustering 0.0260 (0.0170)\t\n",
      "Epoch: [1][190/235]\tLoss 0.0438 (0.0324)\tLoss_recovery 0.0170 (0.0148)\tLoss clustering 0.0268 (0.0175)\t\n",
      "Epoch: [1][200/235]\tLoss 0.0421 (0.0329)\tLoss_recovery 0.0155 (0.0150)\tLoss clustering 0.0267 (0.0180)\t\n",
      "Epoch: [1][210/235]\tLoss 0.0427 (0.0334)\tLoss_recovery 0.0167 (0.0150)\tLoss clustering 0.0260 (0.0183)\t\n",
      "Epoch: [1][220/235]\tLoss 0.0422 (0.0338)\tLoss_recovery 0.0165 (0.0151)\tLoss clustering 0.0256 (0.0187)\t\n",
      "Epoch: [1][230/235]\tLoss 0.0423 (0.0342)\tLoss_recovery 0.0167 (0.0152)\tLoss clustering 0.0256 (0.0190)\t\n",
      "Loss: 0.0343\tLoss_recovery: 0.0152\tLoss_clustering: 0.0191\n",
      "\n",
      "Epoch 2/1000\n",
      "----------\n",
      "\n",
      "Updating target distribution:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\linear_assignment_.py:124: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMI: 0.79038\tARI: 0.73242\tAcc 0.82898\t\n",
      "Epoch: [2][10/235]\tLoss 0.0459 (0.0465)\tLoss_recovery 0.0161 (0.0166)\tLoss clustering 0.0297 (0.0299)\t\n",
      "Epoch: [2][20/235]\tLoss 0.0454 (0.0463)\tLoss_recovery 0.0166 (0.0167)\tLoss clustering 0.0288 (0.0297)\t\n",
      "Epoch: [2][30/235]\tLoss 0.0455 (0.0460)\tLoss_recovery 0.0167 (0.0166)\tLoss clustering 0.0289 (0.0294)\t\n",
      "Epoch: [2][40/235]\tLoss 0.0455 (0.0459)\tLoss_recovery 0.0169 (0.0168)\tLoss clustering 0.0287 (0.0292)\t\n",
      "Epoch: [2][50/235]\tLoss 0.0470 (0.0457)\tLoss_recovery 0.0180 (0.0167)\tLoss clustering 0.0290 (0.0290)\t\n",
      "Epoch: [2][60/235]\tLoss 0.0454 (0.0455)\tLoss_recovery 0.0171 (0.0167)\tLoss clustering 0.0283 (0.0287)\t\n",
      "Epoch: [2][70/235]\tLoss 0.0428 (0.0453)\tLoss_recovery 0.0162 (0.0168)\tLoss clustering 0.0266 (0.0285)\t\n",
      "Epoch: [2][80/235]\tLoss 0.0429 (0.0451)\tLoss_recovery 0.0156 (0.0168)\tLoss clustering 0.0273 (0.0284)\t\n",
      "\n",
      "Updating target distribution:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\linear_assignment_.py:124: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMI: 0.80054\tARI: 0.74242\tAcc 0.83363\t\n",
      "Epoch: [2][90/235]\tLoss 0.0491 (0.0455)\tLoss_recovery 0.0172 (0.0168)\tLoss clustering 0.0319 (0.0287)\t\n",
      "Epoch: [2][100/235]\tLoss 0.0458 (0.0456)\tLoss_recovery 0.0158 (0.0168)\tLoss clustering 0.0301 (0.0288)\t\n",
      "Epoch: [2][110/235]\tLoss 0.0462 (0.0457)\tLoss_recovery 0.0156 (0.0168)\tLoss clustering 0.0306 (0.0289)\t\n",
      "Epoch: [2][120/235]\tLoss 0.0453 (0.0458)\tLoss_recovery 0.0162 (0.0169)\tLoss clustering 0.0291 (0.0289)\t\n",
      "Epoch: [2][130/235]\tLoss 0.0456 (0.0458)\tLoss_recovery 0.0165 (0.0169)\tLoss clustering 0.0291 (0.0289)\t\n",
      "Epoch: [2][140/235]\tLoss 0.0449 (0.0458)\tLoss_recovery 0.0160 (0.0168)\tLoss clustering 0.0289 (0.0290)\t\n",
      "Epoch: [2][150/235]\tLoss 0.0469 (0.0459)\tLoss_recovery 0.0182 (0.0169)\tLoss clustering 0.0287 (0.0290)\t\n",
      "Epoch: [2][160/235]\tLoss 0.0461 (0.0459)\tLoss_recovery 0.0161 (0.0169)\tLoss clustering 0.0299 (0.0290)\t\n",
      "\n",
      "Updating target distribution:\n",
      "NMI: 0.80646\tARI: 0.74860\tAcc 0.83600\t\n",
      "Label divergence 0.009283333333333333< tol 0.01\n",
      "Reached tolerance threshold. Stopping training.\n",
      "Training complete in 1m 13s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\linear_assignment_.py:124: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if args.mode == 'pretrain' and not args.pretrain:\n",
    "        print(\"Nothing to do :(\")\n",
    "        exit()\n",
    "\n",
    "board = args.tensorboard\n",
    "\n",
    "# Deal with pretraining option and way of showing network path\n",
    "pretrain = args.pretrain\n",
    "net_is_path = True\n",
    "if not pretrain:\n",
    "    try:\n",
    "        int(args.pretrained_net)\n",
    "        idx = args.pretrained_net\n",
    "        net_is_path = False\n",
    "    except:\n",
    "        pass\n",
    "params = {'pretrain': pretrain}\n",
    "\n",
    "# Directories\n",
    "# Create directories structure\n",
    "dirs = ['runs', 'reports', 'nets']\n",
    "list(map(lambda x: os.makedirs(x, exist_ok=True), dirs))\n",
    "\n",
    "# Net architecture\n",
    "model_name = args.net_architecture\n",
    "# Indexing (for automated reports saving) - allows to run many trainings and get all the reports collected\n",
    "if pretrain or (not pretrain and net_is_path):\n",
    "    reports_list = sorted(os.listdir('reports'), reverse=True)\n",
    "    if reports_list:\n",
    "        for file in reports_list:\n",
    "            # print(file)\n",
    "            if fnmatch.fnmatch(file, model_name + '*'):\n",
    "                idx = int(str(file)[-7:-4]) + 1\n",
    "                break\n",
    "    try:\n",
    "        idx\n",
    "    except NameError:\n",
    "        idx = 1\n",
    "\n",
    "# Base filename\n",
    "name = model_name + '_' + str(idx).zfill(3)\n",
    "\n",
    "# Filenames for report and weights\n",
    "name_txt = name + '.txt'\n",
    "name_net = name\n",
    "pretrained = name + '_pretrained.pt'\n",
    "\n",
    "# Arrange filenames for report, network weights, pretrained network weights\n",
    "name_txt = os.path.join('reports', name_txt)\n",
    "name_net = os.path.join('nets', name_net)\n",
    "if net_is_path and not pretrain:\n",
    "    pretrained = args.pretrained_net\n",
    "else:\n",
    "    pretrained = os.path.join('nets', pretrained)\n",
    "if not pretrain and not os.path.isfile(pretrained):\n",
    "    print(\"No pretrained weights, try again choosing pretrained network or create new with pretrain=True\")\n",
    "\n",
    "model_files = [name_net, pretrained]\n",
    "params['model_files'] = model_files\n",
    "\n",
    "# Open file\n",
    "if pretrain:\n",
    "    f = open(name_txt, 'w')\n",
    "else:\n",
    "    f = open(name_txt, 'a')\n",
    "params['txt_file'] = f\n",
    "\n",
    "# Delete tensorboard entry if exist (not to overlap as the charts become unreadable)\n",
    "try:\n",
    "    os.system(\"rm -rf runs/\" + name)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Initialize tensorboard writer\n",
    "if board:\n",
    "    writer = SummaryWriter('runs/' + name)\n",
    "    params['writer'] = writer\n",
    "else:\n",
    "    params['writer'] = None\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "# Used dataset\n",
    "dataset = args.dataset\n",
    "\n",
    "# Batch size\n",
    "batch = args.batch_size\n",
    "params['batch'] = batch\n",
    "# Number of workers (typically 4*num_of_GPUs)\n",
    "workers = 4\n",
    "# Learning rate\n",
    "rate = args.rate\n",
    "rate_pretrain = args.rate_pretrain\n",
    "# Adam params\n",
    "# Weight decay\n",
    "weight = args.weight\n",
    "weight_pretrain = args.weight_pretrain\n",
    "# Scheduler steps for rate update\n",
    "sched_step = args.sched_step\n",
    "sched_step_pretrain = args.sched_step_pretrain\n",
    "# Scheduler gamma - multiplier for learning rate\n",
    "sched_gamma = args.sched_gamma\n",
    "sched_gamma_pretrain = args.sched_gamma_pretrain\n",
    "\n",
    "# Number of epochs\n",
    "epochs = args.epochs\n",
    "pretrain_epochs = args.epochs_pretrain\n",
    "params['pretrain_epochs'] = pretrain_epochs\n",
    "\n",
    "# Printing frequency\n",
    "print_freq = args.printing_frequency\n",
    "params['print_freq'] = print_freq\n",
    "\n",
    "# Clustering loss weight:\n",
    "gamma = args.gamma\n",
    "params['gamma'] = gamma\n",
    "\n",
    "# Update interval for target distribution:\n",
    "update_interval = args.update_interval\n",
    "params['update_interval'] = update_interval\n",
    "\n",
    "# Tolerance for label changes:\n",
    "tol = args.tol\n",
    "params['tol'] = tol\n",
    "\n",
    "# Number of clusters\n",
    "num_clusters = args.num_clusters\n",
    "\n",
    "# Report for settings\n",
    "tmp = \"Training the '\" + model_name + \"' architecture\"\n",
    "utils.print_both(f, tmp)\n",
    "tmp = \"\\n\" + \"The following parameters are used:\"\n",
    "utils.print_both(f, tmp)\n",
    "tmp = \"Batch size:\\t\" + str(batch)\n",
    "utils.print_both(f, tmp)\n",
    "tmp = \"Number of workers:\\t\" + str(workers)\n",
    "utils.print_both(f, tmp)\n",
    "tmp = \"Learning rate:\\t\" + str(rate)\n",
    "utils.print_both(f, tmp)\n",
    "tmp = \"Pretraining learning rate:\\t\" + str(rate_pretrain)\n",
    "utils.print_both(f, tmp)\n",
    "tmp = \"Weight decay:\\t\" + str(weight)\n",
    "utils.print_both(f, tmp)\n",
    "tmp = \"Pretraining weight decay:\\t\" + str(weight_pretrain)\n",
    "utils.print_both(f, tmp)\n",
    "tmp = \"Scheduler steps:\\t\" + str(sched_step)\n",
    "utils.print_both(f, tmp)\n",
    "tmp = \"Scheduler gamma:\\t\" + str(sched_gamma)\n",
    "utils.print_both(f, tmp)\n",
    "tmp = \"Pretraining scheduler steps:\\t\" + str(sched_step_pretrain)\n",
    "utils.print_both(f, tmp)\n",
    "tmp = \"Pretraining scheduler gamma:\\t\" + str(sched_gamma_pretrain)\n",
    "utils.print_both(f, tmp)\n",
    "tmp = \"Number of epochs of training:\\t\" + str(epochs)\n",
    "utils.print_both(f, tmp)\n",
    "tmp = \"Number of epochs of pretraining:\\t\" + str(pretrain_epochs)\n",
    "utils.print_both(f, tmp)\n",
    "tmp = \"Clustering loss weight:\\t\" + str(gamma)\n",
    "utils.print_both(f, tmp)\n",
    "tmp = \"Update interval for target distribution:\\t\" + str(update_interval)\n",
    "utils.print_both(f, tmp)\n",
    "tmp = \"Stop criterium tolerance:\\t\" + str(tol)\n",
    "utils.print_both(f, tmp)\n",
    "tmp = \"Number of clusters:\\t\" + str(num_clusters)\n",
    "utils.print_both(f, tmp)\n",
    "tmp = \"Leaky relu:\\t\" + str(args.leaky)\n",
    "utils.print_both(f, tmp)\n",
    "tmp = \"Leaky slope:\\t\" + str(args.neg_slope)\n",
    "utils.print_both(f, tmp)\n",
    "tmp = \"Activations:\\t\" + str(args.activations)\n",
    "utils.print_both(f, tmp)\n",
    "tmp = \"Bias:\\t\" + str(args.bias)\n",
    "utils.print_both(f, tmp)\n",
    "\n",
    "# Data preparation\n",
    "if dataset == 'MNIST-train':\n",
    "    # Uses slightly modified torchvision MNIST class\n",
    "    import mnist\n",
    "    tmp = \"\\nData preparation\\nReading data from: MNIST train dataset\"\n",
    "    utils.print_both(f, tmp)\n",
    "    img_size = [28, 28, 1]\n",
    "    tmp = \"Image size used:\\t{0}x{1}\".format(img_size[0], img_size[1])\n",
    "    utils.print_both(f, tmp)\n",
    "\n",
    "    dataset = mnist.MNIST('../data', train=True, download=True,\n",
    "                          transform=transforms.Compose([\n",
    "                                                       transforms.ToTensor(),\n",
    "                                                       # transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                                       ]))\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(dataset,\n",
    "        batch_size=batch, shuffle=False, num_workers=workers)\n",
    "\n",
    "    dataset_size = len(dataset)\n",
    "    tmp = \"Training set size:\\t\" + str(dataset_size)\n",
    "    utils.print_both(f, tmp)\n",
    "\n",
    "elif dataset == 'MNIST-test':\n",
    "    import mnist\n",
    "    tmp = \"\\nData preparation\\nReading data from: MNIST test dataset\"\n",
    "    utils.print_both(f, tmp)\n",
    "    img_size = [28, 28, 1]\n",
    "    tmp = \"Image size used:\\t{0}x{1}\".format(img_size[0], img_size[1])\n",
    "    utils.print_both(f, tmp)\n",
    "\n",
    "    dataset = mnist.MNIST('../data', train=False, download=True,\n",
    "                          transform=transforms.Compose([\n",
    "                              transforms.ToTensor(),\n",
    "                              # transforms.Normalize((0.1307,), (0.3081,))\n",
    "                          ]))\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                             batch_size=batch, shuffle=False, num_workers=workers)\n",
    "\n",
    "    dataset_size = len(dataset)\n",
    "    tmp = \"Training set size:\\t\" + str(dataset_size)\n",
    "    utils.print_both(f, tmp)\n",
    "\n",
    "elif dataset == 'MNIST-full':\n",
    "    import mnist\n",
    "    tmp = \"\\nData preparation\\nReading data from: MNIST full dataset\"\n",
    "    utils.print_both(f, tmp)\n",
    "    img_size = [28, 28, 1]\n",
    "    tmp = \"Image size used:\\t{0}x{1}\".format(img_size[0], img_size[1])\n",
    "    utils.print_both(f, tmp)\n",
    "\n",
    "    dataset = mnist.MNIST('../data', full=True, download=True,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.ToTensor(),\n",
    "                               # transforms.Normalize((0.1307,), (0.3081,))\n",
    "                           ]))\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                             batch_size=batch, shuffle=False, num_workers=workers)\n",
    "\n",
    "    dataset_size = len(dataset)\n",
    "    tmp = \"Training set size:\\t\" + str(dataset_size)\n",
    "    utils.print_both(f, tmp)\n",
    "\n",
    "else:\n",
    "    # Data folder\n",
    "    data_dir = args.dataset_path\n",
    "    tmp = \"\\nData preparation\\nReading data from:\\t./\" + data_dir\n",
    "    utils.print_both(f, tmp)\n",
    "\n",
    "    # Image size\n",
    "    custom_size = math.nan\n",
    "    custom_size = args.custom_img_size\n",
    "    if isinstance(custom_size, list):\n",
    "        img_size = custom_size\n",
    "\n",
    "    tmp = \"Image size used:\\t{0}x{1}\".format(img_size[0], img_size[1])\n",
    "    utils.print_both(f, tmp)\n",
    "\n",
    "    # Transformations\n",
    "    data_transforms = transforms.Compose([\n",
    "            transforms.Resize(img_size[0:2]),\n",
    "            # transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    # Read data from selected folder and apply transformations\n",
    "    image_dataset = datasets.ImageFolder(data_dir, data_transforms)\n",
    "    # Prepare data for network: schuffle and arrange batches\n",
    "    dataloader = torch.utils.data.DataLoader(image_dataset, batch_size=batch,\n",
    "                                                  shuffle=False, num_workers=workers)\n",
    "\n",
    "    # Size of data sets\n",
    "    dataset_size = len(image_dataset)\n",
    "    tmp = \"Training set size:\\t\" + str(dataset_size)\n",
    "    utils.print_both(f, tmp)\n",
    "\n",
    "params['dataset_size'] = dataset_size\n",
    "\n",
    "# GPU check\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "tmp = \"\\nPerforming calculations on:\\t\" + str(device)\n",
    "utils.print_both(f, tmp + '\\n')\n",
    "params['device'] = device\n",
    "\n",
    "# Evaluate the proper model\n",
    "to_eval = \"nets.\" + model_name + \"(img_size, num_clusters=num_clusters, leaky = args.leaky, neg_slope = args.neg_slope)\"\n",
    "model = eval(to_eval)\n",
    "\n",
    "# Tensorboard model representation\n",
    "# if board:\n",
    "#     writer.add_graph(model, torch.autograd.Variable(torch.Tensor(batch, img_size[2], img_size[0], img_size[1])))\n",
    "\n",
    "model = model.to(device)\n",
    "# Reconstruction loss\n",
    "criterion_1 = nn.MSELoss(size_average=True)\n",
    "# Clustering loss\n",
    "criterion_2 = nn.KLDivLoss(size_average=False)\n",
    "\n",
    "criterion_3 = \n",
    "\n",
    "criteria = [criterion_1, criterion_2]\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=rate, weight_decay=weight)\n",
    "\n",
    "optimizer_pretrain = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=rate_pretrain, weight_decay=weight_pretrain)\n",
    "\n",
    "optimizers = [optimizer, optimizer_pretrain]\n",
    "\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=sched_step, gamma=sched_gamma)\n",
    "scheduler_pretrain = lr_scheduler.StepLR(optimizer_pretrain, step_size=sched_step_pretrain, gamma=sched_gamma_pretrain)\n",
    "\n",
    "schedulers = [scheduler, scheduler_pretrain]\n",
    "\n",
    "if args.mode == 'train_full':\n",
    "    model = training_functions.train_model(model, dataloader, criteria, optimizers, schedulers, epochs, params)\n",
    "elif args.mode == 'pretrain':\n",
    "    model = training_functions.pretraining(model, dataloader, criteria[0], optimizers[1], schedulers[1], epochs, params)\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), name_net + '.pt')\n",
    "\n",
    "# Close files\n",
    "f.close()\n",
    "if board:\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn==0.22.2"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an EnvironmentError: [WinError 5] Access is denied: 'c:\\\\programdata\\\\anaconda3\\\\lib\\\\site-packages\\\\scikit_learn-0.23.2.dist-info\\\\COPYING'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading scikit_learn-0.22.2-cp38-cp38-win_amd64.whl (6.6 MB)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn==0.22.2) (0.17.0)\n",
      "Requirement already satisfied: scipy>=0.17.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn==0.22.2) (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn==0.22.2) (1.19.2)\n",
      "Installing collected packages: scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.23.2\n",
      "    Uninstalling scikit-learn-0.23.2:\n"
     ]
    }
   ],
   "source": [
    "!pip3 install scikit-learn==0.22.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
