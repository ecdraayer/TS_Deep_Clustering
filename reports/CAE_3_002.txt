Training the 'CAE_3' architecture

The following parameters are used:
Batch size:	256
Number of workers:	4
Learning rate:	0.001
Pretraining learning rate:	0.001
Weight decay:	0.0
Pretraining weight decay:	0.0
Scheduler steps:	200
Scheduler gamma:	0.1
Pretraining scheduler steps:	200
Pretraining scheduler gamma:	0.1
Number of epochs of training:	1000
Number of epochs of pretraining:	20
Clustering loss weight:	0.1
Update interval for target distribution:	80
Stop criterium tolerance:	0.01
Number of clusters:	10
Leaky relu:	True
Leaky slope:	0.01
Activations:	False
Bias:	True

Data preparation
Reading data from: MNIST train dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0

Pretraining:	Epoch 1/20
----------
Pretraining:	Epoch: [1][10/235]	Loss 0.0745 (0.0844)	
Pretraining:	Epoch: [1][20/235]	Loss 0.0687 (0.0771)	
Pretraining:	Epoch: [1][30/235]	Loss 0.0607 (0.0727)	
Pretraining:	Epoch: [1][40/235]	Loss 0.0576 (0.0698)	
Pretraining:	Epoch: [1][50/235]	Loss 0.0565 (0.0671)	
Pretraining:	Epoch: [1][60/235]	Loss 0.0470 (0.0645)	
Pretraining:	Epoch: [1][70/235]	Loss 0.0399 (0.0616)	
Pretraining:	Epoch: [1][80/235]	Loss 0.0369 (0.0589)	
Pretraining:	Epoch: [1][90/235]	Loss 0.0355 (0.0564)	
Pretraining:	Epoch: [1][100/235]	Loss 0.0294 (0.0541)	
Pretraining:	Epoch: [1][110/235]	Loss 0.0279 (0.0520)	
Pretraining:	Epoch: [1][120/235]	Loss 0.0292 (0.0502)	
Pretraining:	Epoch: [1][130/235]	Loss 0.0281 (0.0485)	
Pretraining:	Epoch: [1][140/235]	Loss 0.0272 (0.0470)	
Pretraining:	Epoch: [1][150/235]	Loss 0.0270 (0.0457)	
Pretraining:	Epoch: [1][160/235]	Loss 0.0247 (0.0445)	
Pretraining:	Epoch: [1][170/235]	Loss 0.0236 (0.0433)	
Pretraining:	Epoch: [1][180/235]	Loss 0.0250 (0.0423)	
Pretraining:	Epoch: [1][190/235]	Loss 0.0244 (0.0414)	
Pretraining:	Epoch: [1][200/235]	Loss 0.0226 (0.0405)	
Pretraining:	Epoch: [1][210/235]	Loss 0.0231 (0.0397)	
Pretraining:	Epoch: [1][220/235]	Loss 0.0226 (0.0389)	
Pretraining:	Epoch: [1][230/235]	Loss 0.0222 (0.0381)	
Pretraining:	 Loss: 0.0378

Pretraining:	Epoch 2/20
----------
Pretraining:	Epoch: [2][10/235]	Loss 0.0203 (0.0214)	
Pretraining:	Epoch: [2][20/235]	Loss 0.0209 (0.0214)	
Pretraining:	Epoch: [2][30/235]	Loss 0.0203 (0.0212)	
Pretraining:	Epoch: [2][40/235]	Loss 0.0207 (0.0212)	
Pretraining:	Epoch: [2][50/235]	Loss 0.0254 (0.0215)	
Pretraining:	Epoch: [2][60/235]	Loss 0.0214 (0.0215)	
Pretraining:	Epoch: [2][70/235]	Loss 0.0195 (0.0215)	
Pretraining:	Epoch: [2][80/235]	Loss 0.0186 (0.0213)	
Pretraining:	Epoch: [2][90/235]	Loss 0.0201 (0.0212)	
Pretraining:	Epoch: [2][100/235]	Loss 0.0181 (0.0210)	
Pretraining:	Epoch: [2][110/235]	Loss 0.0185 (0.0209)	
Pretraining:	Epoch: [2][120/235]	Loss 0.0188 (0.0208)	
Pretraining:	Epoch: [2][130/235]	Loss 0.0186 (0.0207)	
Pretraining:	Epoch: [2][140/235]	Loss 0.0182 (0.0205)	
Pretraining:	Epoch: [2][150/235]	Loss 0.0197 (0.0205)	
Pretraining:	Epoch: [2][160/235]	Loss 0.0176 (0.0204)	
Pretraining:	Epoch: [2][170/235]	Loss 0.0175 (0.0203)	
Pretraining:	Epoch: [2][180/235]	Loss 0.0195 (0.0202)	
Pretraining:	Epoch: [2][190/235]	Loss 0.0190 (0.0201)	
Pretraining:	Epoch: [2][200/235]	Loss 0.0175 (0.0201)	
Pretraining:	Epoch: [2][210/235]	Loss 0.0183 (0.0200)	
Pretraining:	Epoch: [2][220/235]	Loss 0.0184 (0.0199)	
Pretraining:	Epoch: [2][230/235]	Loss 0.0179 (0.0198)	
Pretraining:	 Loss: 0.0197

Pretraining:	Epoch 3/20
----------
Pretraining:	Epoch: [3][10/235]	Loss 0.0164 (0.0175)	
Pretraining:	Epoch: [3][20/235]	Loss 0.0176 (0.0176)	
Pretraining:	Epoch: [3][30/235]	Loss 0.0170 (0.0175)	
Pretraining:	Epoch: [3][40/235]	Loss 0.0176 (0.0176)	
Pretraining:	Epoch: [3][50/235]	Loss 0.0194 (0.0174)	
Pretraining:	Epoch: [3][60/235]	Loss 0.0178 (0.0175)	
Pretraining:	Epoch: [3][70/235]	Loss 0.0167 (0.0176)	
Pretraining:	Epoch: [3][80/235]	Loss 0.0157 (0.0175)	
Pretraining:	Epoch: [3][90/235]	Loss 0.0174 (0.0175)	
Pretraining:	Epoch: [3][100/235]	Loss 0.0159 (0.0174)	
Pretraining:	Epoch: [3][110/235]	Loss 0.0160 (0.0174)	
Pretraining:	Epoch: [3][120/235]	Loss 0.0168 (0.0174)	
Pretraining:	Epoch: [3][130/235]	Loss 0.0165 (0.0174)	
Pretraining:	Epoch: [3][140/235]	Loss 0.0161 (0.0173)	
Pretraining:	Epoch: [3][150/235]	Loss 0.0178 (0.0173)	
Pretraining:	Epoch: [3][160/235]	Loss 0.0156 (0.0173)	
Pretraining:	Epoch: [3][170/235]	Loss 0.0157 (0.0173)	
Pretraining:	Epoch: [3][180/235]	Loss 0.0175 (0.0172)	
Pretraining:	Epoch: [3][190/235]	Loss 0.0171 (0.0172)	
Pretraining:	Epoch: [3][200/235]	Loss 0.0154 (0.0172)	
Pretraining:	Epoch: [3][210/235]	Loss 0.0164 (0.0172)	
Pretraining:	Epoch: [3][220/235]	Loss 0.0167 (0.0171)	
Pretraining:	Epoch: [3][230/235]	Loss 0.0162 (0.0171)	
Pretraining:	 Loss: 0.0170

Pretraining:	Epoch 4/20
----------
Pretraining:	Epoch: [4][10/235]	Loss 0.0149 (0.0157)	
Pretraining:	Epoch: [4][20/235]	Loss 0.0159 (0.0158)	
Pretraining:	Epoch: [4][30/235]	Loss 0.0155 (0.0158)	
Pretraining:	Epoch: [4][40/235]	Loss 0.0162 (0.0159)	
Pretraining:	Epoch: [4][50/235]	Loss 0.0178 (0.0159)	
Pretraining:	Epoch: [4][60/235]	Loss 0.0164 (0.0160)	
Pretraining:	Epoch: [4][70/235]	Loss 0.0153 (0.0160)	
Pretraining:	Epoch: [4][80/235]	Loss 0.0144 (0.0159)	
Pretraining:	Epoch: [4][90/235]	Loss 0.0162 (0.0159)	
Pretraining:	Epoch: [4][100/235]	Loss 0.0149 (0.0159)	
Pretraining:	Epoch: [4][110/235]	Loss 0.0147 (0.0159)	
Pretraining:	Epoch: [4][120/235]	Loss 0.0153 (0.0159)	
Pretraining:	Epoch: [4][130/235]	Loss 0.0153 (0.0159)	
Pretraining:	Epoch: [4][140/235]	Loss 0.0147 (0.0159)	
Pretraining:	Epoch: [4][150/235]	Loss 0.0167 (0.0159)	
Pretraining:	Epoch: [4][160/235]	Loss 0.0143 (0.0159)	
Pretraining:	Epoch: [4][170/235]	Loss 0.0146 (0.0158)	
Pretraining:	Epoch: [4][180/235]	Loss 0.0164 (0.0158)	
Pretraining:	Epoch: [4][190/235]	Loss 0.0160 (0.0158)	
Pretraining:	Epoch: [4][200/235]	Loss 0.0143 (0.0158)	
Pretraining:	Epoch: [4][210/235]	Loss 0.0153 (0.0158)	
Pretraining:	Epoch: [4][220/235]	Loss 0.0156 (0.0158)	
Pretraining:	Epoch: [4][230/235]	Loss 0.0151 (0.0157)	
Pretraining:	 Loss: 0.0157

Pretraining:	Epoch 5/20
----------
Pretraining:	Epoch: [5][10/235]	Loss 0.0141 (0.0147)	
Pretraining:	Epoch: [5][20/235]	Loss 0.0151 (0.0149)	
Pretraining:	Epoch: [5][30/235]	Loss 0.0146 (0.0149)	
Pretraining:	Epoch: [5][40/235]	Loss 0.0154 (0.0150)	
Pretraining:	Epoch: [5][50/235]	Loss 0.0169 (0.0150)	
Pretraining:	Epoch: [5][60/235]	Loss 0.0160 (0.0151)	
Pretraining:	Epoch: [5][70/235]	Loss 0.0146 (0.0152)	
Pretraining:	Epoch: [5][80/235]	Loss 0.0137 (0.0151)	
Pretraining:	Epoch: [5][90/235]	Loss 0.0196 (0.0152)	
Pretraining:	Epoch: [5][100/235]	Loss 0.0143 (0.0154)	
Pretraining:	Epoch: [5][110/235]	Loss 0.0141 (0.0154)	
Pretraining:	Epoch: [5][120/235]	Loss 0.0147 (0.0154)	
Pretraining:	Epoch: [5][130/235]	Loss 0.0147 (0.0154)	
Pretraining:	Epoch: [5][140/235]	Loss 0.0140 (0.0153)	
Pretraining:	Epoch: [5][150/235]	Loss 0.0160 (0.0154)	
Pretraining:	Epoch: [5][160/235]	Loss 0.0136 (0.0153)	
Pretraining:	Epoch: [5][170/235]	Loss 0.0139 (0.0153)	
Pretraining:	Epoch: [5][180/235]	Loss 0.0156 (0.0153)	
Pretraining:	Epoch: [5][190/235]	Loss 0.0153 (0.0153)	
Pretraining:	Epoch: [5][200/235]	Loss 0.0135 (0.0153)	
Pretraining:	Epoch: [5][210/235]	Loss 0.0146 (0.0152)	
Pretraining:	Epoch: [5][220/235]	Loss 0.0150 (0.0152)	
Pretraining:	Epoch: [5][230/235]	Loss 0.0144 (0.0151)	
Pretraining:	 Loss: 0.0151

Pretraining:	Epoch 6/20
----------
Pretraining:	Epoch: [6][10/235]	Loss 0.0134 (0.0141)	
Pretraining:	Epoch: [6][20/235]	Loss 0.0145 (0.0143)	
Pretraining:	Epoch: [6][30/235]	Loss 0.0139 (0.0143)	
Pretraining:	Epoch: [6][40/235]	Loss 0.0146 (0.0144)	
Pretraining:	Epoch: [6][50/235]	Loss 0.0162 (0.0143)	
Pretraining:	Epoch: [6][60/235]	Loss 0.0147 (0.0144)	
Pretraining:	Epoch: [6][70/235]	Loss 0.0139 (0.0145)	
Pretraining:	Epoch: [6][80/235]	Loss 0.0132 (0.0144)	
Pretraining:	Epoch: [6][90/235]	Loss 0.0149 (0.0144)	
Pretraining:	Epoch: [6][100/235]	Loss 0.0133 (0.0144)	
Pretraining:	Epoch: [6][110/235]	Loss 0.0134 (0.0144)	
Pretraining:	Epoch: [6][120/235]	Loss 0.0141 (0.0144)	
Pretraining:	Epoch: [6][130/235]	Loss 0.0139 (0.0144)	
Pretraining:	Epoch: [6][140/235]	Loss 0.0134 (0.0144)	
Pretraining:	Epoch: [6][150/235]	Loss 0.0155 (0.0144)	
Pretraining:	Epoch: [6][160/235]	Loss 0.0132 (0.0144)	
Pretraining:	Epoch: [6][170/235]	Loss 0.0134 (0.0144)	
Pretraining:	Epoch: [6][180/235]	Loss 0.0151 (0.0144)	
Pretraining:	Epoch: [6][190/235]	Loss 0.0148 (0.0144)	
Pretraining:	Epoch: [6][200/235]	Loss 0.0129 (0.0144)	
Pretraining:	Epoch: [6][210/235]	Loss 0.0140 (0.0144)	
Pretraining:	Epoch: [6][220/235]	Loss 0.0144 (0.0144)	
Pretraining:	Epoch: [6][230/235]	Loss 0.0139 (0.0143)	
Pretraining:	 Loss: 0.0143

Pretraining:	Epoch 7/20
----------
Pretraining:	Epoch: [7][10/235]	Loss 0.0130 (0.0136)	
Pretraining:	Epoch: [7][20/235]	Loss 0.0140 (0.0138)	
Pretraining:	Epoch: [7][30/235]	Loss 0.0134 (0.0138)	
Pretraining:	Epoch: [7][40/235]	Loss 0.0139 (0.0139)	
Pretraining:	Epoch: [7][50/235]	Loss 0.0156 (0.0138)	
Pretraining:	Epoch: [7][60/235]	Loss 0.0142 (0.0139)	
Pretraining:	Epoch: [7][70/235]	Loss 0.0134 (0.0140)	
Pretraining:	Epoch: [7][80/235]	Loss 0.0126 (0.0139)	
Pretraining:	Epoch: [7][90/235]	Loss 0.0146 (0.0139)	
Pretraining:	Epoch: [7][100/235]	Loss 0.0129 (0.0139)	
Pretraining:	Epoch: [7][110/235]	Loss 0.0130 (0.0139)	
Pretraining:	Epoch: [7][120/235]	Loss 0.0136 (0.0139)	
Pretraining:	Epoch: [7][130/235]	Loss 0.0134 (0.0139)	
Pretraining:	Epoch: [7][140/235]	Loss 0.0129 (0.0139)	
Pretraining:	Epoch: [7][150/235]	Loss 0.0150 (0.0140)	
Pretraining:	Epoch: [7][160/235]	Loss 0.0127 (0.0139)	
Pretraining:	Epoch: [7][170/235]	Loss 0.0130 (0.0139)	
Pretraining:	Epoch: [7][180/235]	Loss 0.0146 (0.0139)	
Pretraining:	Epoch: [7][190/235]	Loss 0.0143 (0.0139)	
Pretraining:	Epoch: [7][200/235]	Loss 0.0124 (0.0140)	
Pretraining:	Epoch: [7][210/235]	Loss 0.0136 (0.0139)	
Pretraining:	Epoch: [7][220/235]	Loss 0.0140 (0.0139)	
Pretraining:	Epoch: [7][230/235]	Loss 0.0136 (0.0139)	
Pretraining:	 Loss: 0.0139

Pretraining:	Epoch 8/20
----------
Pretraining:	Epoch: [8][10/235]	Loss 0.0127 (0.0133)	
Pretraining:	Epoch: [8][20/235]	Loss 0.0137 (0.0134)	
Pretraining:	Epoch: [8][30/235]	Loss 0.0131 (0.0134)	
Pretraining:	Epoch: [8][40/235]	Loss 0.0135 (0.0135)	
Pretraining:	Epoch: [8][50/235]	Loss 0.0152 (0.0135)	
Pretraining:	Epoch: [8][60/235]	Loss 0.0138 (0.0135)	
Pretraining:	Epoch: [8][70/235]	Loss 0.0130 (0.0136)	
Pretraining:	Epoch: [8][80/235]	Loss 0.0122 (0.0135)	
Pretraining:	Epoch: [8][90/235]	Loss 0.0141 (0.0135)	
Pretraining:	Epoch: [8][100/235]	Loss 0.0125 (0.0135)	
Pretraining:	Epoch: [8][110/235]	Loss 0.0125 (0.0135)	
Pretraining:	Epoch: [8][120/235]	Loss 0.0131 (0.0135)	
Pretraining:	Epoch: [8][130/235]	Loss 0.0129 (0.0135)	
Pretraining:	Epoch: [8][140/235]	Loss 0.0126 (0.0135)	
Pretraining:	Epoch: [8][150/235]	Loss 0.0148 (0.0135)	
Pretraining:	Epoch: [8][160/235]	Loss 0.0124 (0.0135)	
Pretraining:	Epoch: [8][170/235]	Loss 0.0127 (0.0135)	
Pretraining:	Epoch: [8][180/235]	Loss 0.0143 (0.0135)	
Pretraining:	Epoch: [8][190/235]	Loss 0.0140 (0.0135)	
Pretraining:	Epoch: [8][200/235]	Loss 0.0121 (0.0135)	
Pretraining:	Epoch: [8][210/235]	Loss 0.0132 (0.0135)	
Pretraining:	Epoch: [8][220/235]	Loss 0.0136 (0.0135)	
Pretraining:	Epoch: [8][230/235]	Loss 0.0132 (0.0135)	
Pretraining:	 Loss: 0.0135

Pretraining:	Epoch 9/20
----------
Pretraining:	Epoch: [9][10/235]	Loss 0.0122 (0.0129)	
Pretraining:	Epoch: [9][20/235]	Loss 0.0133 (0.0130)	
Pretraining:	Epoch: [9][30/235]	Loss 0.0128 (0.0130)	
Pretraining:	Epoch: [9][40/235]	Loss 0.0132 (0.0132)	
Pretraining:	Epoch: [9][50/235]	Loss 0.0148 (0.0131)	
Pretraining:	Epoch: [9][60/235]	Loss 0.0136 (0.0132)	
Pretraining:	Epoch: [9][70/235]	Loss 0.0127 (0.0132)	
Pretraining:	Epoch: [9][80/235]	Loss 0.0119 (0.0132)	
Pretraining:	Epoch: [9][90/235]	Loss 0.0138 (0.0132)	
Pretraining:	Epoch: [9][100/235]	Loss 0.0122 (0.0132)	
Pretraining:	Epoch: [9][110/235]	Loss 0.0123 (0.0132)	
Pretraining:	Epoch: [9][120/235]	Loss 0.0128 (0.0132)	
Pretraining:	Epoch: [9][130/235]	Loss 0.0126 (0.0132)	
Pretraining:	Epoch: [9][140/235]	Loss 0.0123 (0.0132)	
Pretraining:	Epoch: [9][150/235]	Loss 0.0145 (0.0132)	
Pretraining:	Epoch: [9][160/235]	Loss 0.0121 (0.0132)	
Pretraining:	Epoch: [9][170/235]	Loss 0.0125 (0.0132)	
Pretraining:	Epoch: [9][180/235]	Loss 0.0140 (0.0132)	
Pretraining:	Epoch: [9][190/235]	Loss 0.0136 (0.0132)	
Pretraining:	Epoch: [9][200/235]	Loss 0.0117 (0.0132)	
Pretraining:	Epoch: [9][210/235]	Loss 0.0129 (0.0132)	
Pretraining:	Epoch: [9][220/235]	Loss 0.0133 (0.0132)	
Pretraining:	Epoch: [9][230/235]	Loss 0.0129 (0.0132)	
Pretraining:	 Loss: 0.0132

Pretraining:	Epoch 10/20
----------
Pretraining:	Epoch: [10][10/235]	Loss 0.0119 (0.0126)	
Pretraining:	Epoch: [10][20/235]	Loss 0.0130 (0.0128)	
Pretraining:	Epoch: [10][30/235]	Loss 0.0125 (0.0128)	
Pretraining:	Epoch: [10][40/235]	Loss 0.0129 (0.0129)	
Pretraining:	Epoch: [10][50/235]	Loss 0.0144 (0.0128)	
Pretraining:	Epoch: [10][60/235]	Loss 0.0134 (0.0129)	
Pretraining:	Epoch: [10][70/235]	Loss 0.0125 (0.0129)	
Pretraining:	Epoch: [10][80/235]	Loss 0.0117 (0.0129)	
Pretraining:	Epoch: [10][90/235]	Loss 0.0134 (0.0129)	
Pretraining:	Epoch: [10][100/235]	Loss 0.0119 (0.0129)	
Pretraining:	Epoch: [10][110/235]	Loss 0.0120 (0.0129)	
Pretraining:	Epoch: [10][120/235]	Loss 0.0125 (0.0129)	
Pretraining:	Epoch: [10][130/235]	Loss 0.0124 (0.0129)	
Pretraining:	Epoch: [10][140/235]	Loss 0.0120 (0.0129)	
Pretraining:	Epoch: [10][150/235]	Loss 0.0143 (0.0129)	
Pretraining:	Epoch: [10][160/235]	Loss 0.0119 (0.0129)	
Pretraining:	Epoch: [10][170/235]	Loss 0.0123 (0.0129)	
Pretraining:	Epoch: [10][180/235]	Loss 0.0137 (0.0129)	
Pretraining:	Epoch: [10][190/235]	Loss 0.0134 (0.0129)	
Pretraining:	Epoch: [10][200/235]	Loss 0.0115 (0.0129)	
Pretraining:	Epoch: [10][210/235]	Loss 0.0127 (0.0129)	
Pretraining:	Epoch: [10][220/235]	Loss 0.0131 (0.0129)	
Pretraining:	Epoch: [10][230/235]	Loss 0.0126 (0.0129)	
Pretraining:	 Loss: 0.0129

Pretraining:	Epoch 11/20
----------
Pretraining:	Epoch: [11][10/235]	Loss 0.0116 (0.0124)	
Pretraining:	Epoch: [11][20/235]	Loss 0.0128 (0.0125)	
Pretraining:	Epoch: [11][30/235]	Loss 0.0122 (0.0125)	
Pretraining:	Epoch: [11][40/235]	Loss 0.0127 (0.0126)	
Pretraining:	Epoch: [11][50/235]	Loss 0.0141 (0.0126)	
Pretraining:	Epoch: [11][60/235]	Loss 0.0131 (0.0127)	
Pretraining:	Epoch: [11][70/235]	Loss 0.0123 (0.0127)	
Pretraining:	Epoch: [11][80/235]	Loss 0.0115 (0.0126)	
Pretraining:	Epoch: [11][90/235]	Loss 0.0131 (0.0127)	
Pretraining:	Epoch: [11][100/235]	Loss 0.0117 (0.0126)	
Pretraining:	Epoch: [11][110/235]	Loss 0.0117 (0.0127)	
Pretraining:	Epoch: [11][120/235]	Loss 0.0122 (0.0127)	
Pretraining:	Epoch: [11][130/235]	Loss 0.0122 (0.0127)	
Pretraining:	Epoch: [11][140/235]	Loss 0.0117 (0.0126)	
Pretraining:	Epoch: [11][150/235]	Loss 0.0141 (0.0127)	
Pretraining:	Epoch: [11][160/235]	Loss 0.0117 (0.0127)	
Pretraining:	Epoch: [11][170/235]	Loss 0.0121 (0.0127)	
Pretraining:	Epoch: [11][180/235]	Loss 0.0135 (0.0127)	
Pretraining:	Epoch: [11][190/235]	Loss 0.0131 (0.0127)	
Pretraining:	Epoch: [11][200/235]	Loss 0.0112 (0.0127)	
Pretraining:	Epoch: [11][210/235]	Loss 0.0124 (0.0127)	
Pretraining:	Epoch: [11][220/235]	Loss 0.0128 (0.0127)	
Pretraining:	Epoch: [11][230/235]	Loss 0.0124 (0.0127)	
Pretraining:	 Loss: 0.0127

Pretraining:	Epoch 12/20
----------
Pretraining:	Epoch: [12][10/235]	Loss 0.0114 (0.0122)	
Pretraining:	Epoch: [12][20/235]	Loss 0.0126 (0.0123)	
Pretraining:	Epoch: [12][30/235]	Loss 0.0120 (0.0123)	
Pretraining:	Epoch: [12][40/235]	Loss 0.0124 (0.0124)	
Pretraining:	Epoch: [12][50/235]	Loss 0.0139 (0.0123)	
Pretraining:	Epoch: [12][60/235]	Loss 0.0128 (0.0124)	
Pretraining:	Epoch: [12][70/235]	Loss 0.0120 (0.0125)	
Pretraining:	Epoch: [12][80/235]	Loss 0.0112 (0.0124)	
Pretraining:	Epoch: [12][90/235]	Loss 0.0128 (0.0124)	
Pretraining:	Epoch: [12][100/235]	Loss 0.0115 (0.0124)	
Pretraining:	Epoch: [12][110/235]	Loss 0.0115 (0.0124)	
Pretraining:	Epoch: [12][120/235]	Loss 0.0119 (0.0124)	
Pretraining:	Epoch: [12][130/235]	Loss 0.0119 (0.0124)	
Pretraining:	Epoch: [12][140/235]	Loss 0.0115 (0.0124)	
Pretraining:	Epoch: [12][150/235]	Loss 0.0139 (0.0124)	
Pretraining:	Epoch: [12][160/235]	Loss 0.0115 (0.0124)	
Pretraining:	Epoch: [12][170/235]	Loss 0.0119 (0.0124)	
Pretraining:	Epoch: [12][180/235]	Loss 0.0133 (0.0124)	
Pretraining:	Epoch: [12][190/235]	Loss 0.0129 (0.0125)	
Pretraining:	Epoch: [12][200/235]	Loss 0.0110 (0.0125)	
Pretraining:	Epoch: [12][210/235]	Loss 0.0122 (0.0125)	
Pretraining:	Epoch: [12][220/235]	Loss 0.0126 (0.0125)	
Pretraining:	Epoch: [12][230/235]	Loss 0.0122 (0.0124)	
Pretraining:	 Loss: 0.0124

Pretraining:	Epoch 13/20
----------
Pretraining:	Epoch: [13][10/235]	Loss 0.0112 (0.0120)	
Pretraining:	Epoch: [13][20/235]	Loss 0.0124 (0.0121)	
Pretraining:	Epoch: [13][30/235]	Loss 0.0118 (0.0121)	
Pretraining:	Epoch: [13][40/235]	Loss 0.0122 (0.0122)	
Pretraining:	Epoch: [13][50/235]	Loss 0.0137 (0.0122)	
Pretraining:	Epoch: [13][60/235]	Loss 0.0127 (0.0122)	
Pretraining:	Epoch: [13][70/235]	Loss 0.0119 (0.0123)	
Pretraining:	Epoch: [13][80/235]	Loss 0.0111 (0.0122)	
Pretraining:	Epoch: [13][90/235]	Loss 0.0127 (0.0122)	
Pretraining:	Epoch: [13][100/235]	Loss 0.0113 (0.0122)	
Pretraining:	Epoch: [13][110/235]	Loss 0.0115 (0.0123)	
Pretraining:	Epoch: [13][120/235]	Loss 0.0118 (0.0123)	
Pretraining:	Epoch: [13][130/235]	Loss 0.0120 (0.0123)	
Pretraining:	Epoch: [13][140/235]	Loss 0.0115 (0.0123)	
Pretraining:	Epoch: [13][150/235]	Loss 0.0140 (0.0123)	
Pretraining:	Epoch: [13][160/235]	Loss 0.0114 (0.0123)	
Pretraining:	Epoch: [13][170/235]	Loss 0.0117 (0.0123)	
Pretraining:	Epoch: [13][180/235]	Loss 0.0131 (0.0123)	
Pretraining:	Epoch: [13][190/235]	Loss 0.0127 (0.0123)	
Pretraining:	Epoch: [13][200/235]	Loss 0.0109 (0.0124)	
Pretraining:	Epoch: [13][210/235]	Loss 0.0120 (0.0123)	
Pretraining:	Epoch: [13][220/235]	Loss 0.0125 (0.0123)	
Pretraining:	Epoch: [13][230/235]	Loss 0.0121 (0.0123)	
Pretraining:	 Loss: 0.0123

Pretraining:	Epoch 14/20
----------
Pretraining:	Epoch: [14][10/235]	Loss 0.0113 (0.0119)	
Pretraining:	Epoch: [14][20/235]	Loss 0.0123 (0.0121)	
Pretraining:	Epoch: [14][30/235]	Loss 0.0116 (0.0120)	
Pretraining:	Epoch: [14][40/235]	Loss 0.0120 (0.0121)	
Pretraining:	Epoch: [14][50/235]	Loss 0.0134 (0.0121)	
Pretraining:	Epoch: [14][60/235]	Loss 0.0123 (0.0121)	
Pretraining:	Epoch: [14][70/235]	Loss 0.0116 (0.0121)	
Pretraining:	Epoch: [14][80/235]	Loss 0.0109 (0.0121)	
Pretraining:	Epoch: [14][90/235]	Loss 0.0126 (0.0121)	
Pretraining:	Epoch: [14][100/235]	Loss 0.0112 (0.0121)	
Pretraining:	Epoch: [14][110/235]	Loss 0.0111 (0.0121)	
Pretraining:	Epoch: [14][120/235]	Loss 0.0116 (0.0121)	
Pretraining:	Epoch: [14][130/235]	Loss 0.0118 (0.0121)	
Pretraining:	Epoch: [14][140/235]	Loss 0.0113 (0.0121)	
Pretraining:	Epoch: [14][150/235]	Loss 0.0136 (0.0121)	
Pretraining:	Epoch: [14][160/235]	Loss 0.0111 (0.0121)	
Pretraining:	Epoch: [14][170/235]	Loss 0.0115 (0.0121)	
Pretraining:	Epoch: [14][180/235]	Loss 0.0128 (0.0121)	
Pretraining:	Epoch: [14][190/235]	Loss 0.0126 (0.0121)	
Pretraining:	Epoch: [14][200/235]	Loss 0.0107 (0.0122)	
Pretraining:	Epoch: [14][210/235]	Loss 0.0118 (0.0121)	
Pretraining:	Epoch: [14][220/235]	Loss 0.0123 (0.0121)	
Pretraining:	Epoch: [14][230/235]	Loss 0.0119 (0.0121)	
Pretraining:	 Loss: 0.0121

Pretraining:	Epoch 15/20
----------
Pretraining:	Epoch: [15][10/235]	Loss 0.0113 (0.0118)	
Pretraining:	Epoch: [15][20/235]	Loss 0.0122 (0.0120)	
Pretraining:	Epoch: [15][30/235]	Loss 0.0116 (0.0119)	
Pretraining:	Epoch: [15][40/235]	Loss 0.0119 (0.0120)	
Pretraining:	Epoch: [15][50/235]	Loss 0.0132 (0.0119)	
Pretraining:	Epoch: [15][60/235]	Loss 0.0121 (0.0120)	
Pretraining:	Epoch: [15][70/235]	Loss 0.0115 (0.0120)	
Pretraining:	Epoch: [15][80/235]	Loss 0.0108 (0.0120)	
Pretraining:	Epoch: [15][90/235]	Loss 0.0122 (0.0120)	
Pretraining:	Epoch: [15][100/235]	Loss 0.0108 (0.0119)	
Pretraining:	Epoch: [15][110/235]	Loss 0.0111 (0.0119)	
Pretraining:	Epoch: [15][120/235]	Loss 0.0118 (0.0120)	
Pretraining:	Epoch: [15][130/235]	Loss 0.0117 (0.0120)	
Pretraining:	Epoch: [15][140/235]	Loss 0.0123 (0.0120)	
Pretraining:	Epoch: [15][150/235]	Loss 0.0139 (0.0121)	
Pretraining:	Epoch: [15][160/235]	Loss 0.0111 (0.0121)	
Pretraining:	Epoch: [15][170/235]	Loss 0.0114 (0.0121)	
Pretraining:	Epoch: [15][180/235]	Loss 0.0127 (0.0121)	
Pretraining:	Epoch: [15][190/235]	Loss 0.0124 (0.0121)	
Pretraining:	Epoch: [15][200/235]	Loss 0.0105 (0.0121)	
Pretraining:	Epoch: [15][210/235]	Loss 0.0117 (0.0121)	
Pretraining:	Epoch: [15][220/235]	Loss 0.0121 (0.0121)	
Pretraining:	Epoch: [15][230/235]	Loss 0.0116 (0.0121)	
Pretraining:	 Loss: 0.0121

Pretraining:	Epoch 16/20
----------
Pretraining:	Epoch: [16][10/235]	Loss 0.0108 (0.0114)	
Pretraining:	Epoch: [16][20/235]	Loss 0.0117 (0.0115)	
Pretraining:	Epoch: [16][30/235]	Loss 0.0111 (0.0115)	
Pretraining:	Epoch: [16][40/235]	Loss 0.0116 (0.0117)	
Pretraining:	Epoch: [16][50/235]	Loss 0.0130 (0.0116)	
Pretraining:	Epoch: [16][60/235]	Loss 0.0120 (0.0117)	
Pretraining:	Epoch: [16][70/235]	Loss 0.0114 (0.0117)	
Pretraining:	Epoch: [16][80/235]	Loss 0.0107 (0.0117)	
Pretraining:	Epoch: [16][90/235]	Loss 0.0123 (0.0117)	
Pretraining:	Epoch: [16][100/235]	Loss 0.0107 (0.0117)	
Pretraining:	Epoch: [16][110/235]	Loss 0.0109 (0.0117)	
Pretraining:	Epoch: [16][120/235]	Loss 0.0113 (0.0118)	
Pretraining:	Epoch: [16][130/235]	Loss 0.0113 (0.0118)	
Pretraining:	Epoch: [16][140/235]	Loss 0.0110 (0.0117)	
Pretraining:	Epoch: [16][150/235]	Loss 0.0132 (0.0118)	
Pretraining:	Epoch: [16][160/235]	Loss 0.0109 (0.0118)	
Pretraining:	Epoch: [16][170/235]	Loss 0.0112 (0.0118)	
Pretraining:	Epoch: [16][180/235]	Loss 0.0125 (0.0118)	
Pretraining:	Epoch: [16][190/235]	Loss 0.0122 (0.0118)	
Pretraining:	Epoch: [16][200/235]	Loss 0.0104 (0.0118)	
Pretraining:	Epoch: [16][210/235]	Loss 0.0115 (0.0118)	
Pretraining:	Epoch: [16][220/235]	Loss 0.0119 (0.0118)	
Pretraining:	Epoch: [16][230/235]	Loss 0.0114 (0.0118)	
Pretraining:	 Loss: 0.0118

Pretraining:	Epoch 17/20
----------
Pretraining:	Epoch: [17][10/235]	Loss 0.0106 (0.0113)	
Pretraining:	Epoch: [17][20/235]	Loss 0.0116 (0.0114)	
Pretraining:	Epoch: [17][30/235]	Loss 0.0109 (0.0114)	
Pretraining:	Epoch: [17][40/235]	Loss 0.0114 (0.0115)	
Pretraining:	Epoch: [17][50/235]	Loss 0.0128 (0.0114)	
Pretraining:	Epoch: [17][60/235]	Loss 0.0118 (0.0115)	
Pretraining:	Epoch: [17][70/235]	Loss 0.0112 (0.0115)	
Pretraining:	Epoch: [17][80/235]	Loss 0.0106 (0.0115)	
Pretraining:	Epoch: [17][90/235]	Loss 0.0121 (0.0115)	
Pretraining:	Epoch: [17][100/235]	Loss 0.0105 (0.0115)	
Pretraining:	Epoch: [17][110/235]	Loss 0.0108 (0.0115)	
Pretraining:	Epoch: [17][120/235]	Loss 0.0112 (0.0116)	
Pretraining:	Epoch: [17][130/235]	Loss 0.0112 (0.0116)	
Pretraining:	Epoch: [17][140/235]	Loss 0.0109 (0.0116)	
Pretraining:	Epoch: [17][150/235]	Loss 0.0130 (0.0116)	
Pretraining:	Epoch: [17][160/235]	Loss 0.0107 (0.0116)	
Pretraining:	Epoch: [17][170/235]	Loss 0.0110 (0.0116)	
Pretraining:	Epoch: [17][180/235]	Loss 0.0124 (0.0116)	
Pretraining:	Epoch: [17][190/235]	Loss 0.0121 (0.0116)	
Pretraining:	Epoch: [17][200/235]	Loss 0.0102 (0.0116)	
Pretraining:	Epoch: [17][210/235]	Loss 0.0114 (0.0116)	
Pretraining:	Epoch: [17][220/235]	Loss 0.0118 (0.0116)	
Pretraining:	Epoch: [17][230/235]	Loss 0.0112 (0.0116)	
Pretraining:	 Loss: 0.0116

Pretraining:	Epoch 18/20
----------
Pretraining:	Epoch: [18][10/235]	Loss 0.0105 (0.0111)	
Pretraining:	Epoch: [18][20/235]	Loss 0.0114 (0.0112)	
Pretraining:	Epoch: [18][30/235]	Loss 0.0108 (0.0112)	
Pretraining:	Epoch: [18][40/235]	Loss 0.0113 (0.0113)	
Pretraining:	Epoch: [18][50/235]	Loss 0.0127 (0.0113)	
Pretraining:	Epoch: [18][60/235]	Loss 0.0117 (0.0113)	
Pretraining:	Epoch: [18][70/235]	Loss 0.0111 (0.0114)	
Pretraining:	Epoch: [18][80/235]	Loss 0.0104 (0.0113)	
Pretraining:	Epoch: [18][90/235]	Loss 0.0119 (0.0113)	
Pretraining:	Epoch: [18][100/235]	Loss 0.0104 (0.0113)	
Pretraining:	Epoch: [18][110/235]	Loss 0.0107 (0.0114)	
Pretraining:	Epoch: [18][120/235]	Loss 0.0111 (0.0114)	
Pretraining:	Epoch: [18][130/235]	Loss 0.0112 (0.0114)	
Pretraining:	Epoch: [18][140/235]	Loss 0.0107 (0.0114)	
