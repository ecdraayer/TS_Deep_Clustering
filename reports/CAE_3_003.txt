Training the 'CAE_3' architecture

The following parameters are used:
Batch size:	256
Number of workers:	4
Learning rate:	0.001
Pretraining learning rate:	0.001
Weight decay:	0.0
Pretraining weight decay:	0.0
Scheduler steps:	200
Scheduler gamma:	0.1
Pretraining scheduler steps:	200
Pretraining scheduler gamma:	0.1
Number of epochs of training:	1000
Number of epochs of pretraining:	20
Clustering loss weight:	0.1
Update interval for target distribution:	80
Stop criterium tolerance:	0.01
Number of clusters:	10
Leaky relu:	True
Leaky slope:	0.01
Activations:	False
Bias:	True

Data preparation
Reading data from: MNIST train dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0

Pretraining:	Epoch 1/20
----------
Pretraining:	Epoch: [1][10/235]	Loss 0.0858 (0.1020)	
Pretraining:	Epoch: [1][20/235]	Loss 0.0753 (0.0901)	
Pretraining:	Epoch: [1][30/235]	Loss 0.0630 (0.0832)	
Pretraining:	Epoch: [1][40/235]	Loss 0.0675 (0.0794)	
Pretraining:	Epoch: [1][50/235]	Loss 0.0667 (0.0767)	
Pretraining:	Epoch: [1][60/235]	Loss 0.0609 (0.0748)	
Pretraining:	Epoch: [1][70/235]	Loss 0.0575 (0.0729)	
Pretraining:	Epoch: [1][80/235]	Loss 0.0590 (0.0715)	
Pretraining:	Epoch: [1][90/235]	Loss 0.0574 (0.0700)	
Pretraining:	Epoch: [1][100/235]	Loss 0.0507 (0.0682)	
Pretraining:	Epoch: [1][110/235]	Loss 0.0443 (0.0663)	
Pretraining:	Epoch: [1][120/235]	Loss 0.0413 (0.0645)	
Pretraining:	Epoch: [1][130/235]	Loss 0.0405 (0.0627)	
Pretraining:	Epoch: [1][140/235]	Loss 0.0365 (0.0609)	
Pretraining:	Epoch: [1][150/235]	Loss 0.0356 (0.0593)	
Pretraining:	Epoch: [1][160/235]	Loss 0.0321 (0.0577)	
Pretraining:	Epoch: [1][170/235]	Loss 0.0306 (0.0562)	
Pretraining:	Epoch: [1][180/235]	Loss 0.0302 (0.0548)	
Pretraining:	Epoch: [1][190/235]	Loss 0.0298 (0.0535)	
Pretraining:	Epoch: [1][200/235]	Loss 0.0277 (0.0522)	
Pretraining:	Epoch: [1][210/235]	Loss 0.0277 (0.0511)	
Pretraining:	Epoch: [1][220/235]	Loss 0.0263 (0.0499)	
Pretraining:	Epoch: [1][230/235]	Loss 0.0254 (0.0489)	
Pretraining:	 Loss: 0.0485

Pretraining:	Epoch 2/20
----------
Pretraining:	Epoch: [2][10/235]	Loss 0.0244 (0.0252)	
Pretraining:	Epoch: [2][20/235]	Loss 0.0243 (0.0250)	
Pretraining:	Epoch: [2][30/235]	Loss 0.0238 (0.0247)	
Pretraining:	Epoch: [2][40/235]	Loss 0.0237 (0.0245)	
Pretraining:	Epoch: [2][50/235]	Loss 0.0259 (0.0243)	
Pretraining:	Epoch: [2][60/235]	Loss 0.0235 (0.0242)	
Pretraining:	Epoch: [2][70/235]	Loss 0.0223 (0.0241)	
Pretraining:	Epoch: [2][80/235]	Loss 0.0212 (0.0239)	
Pretraining:	Epoch: [2][90/235]	Loss 0.0224 (0.0238)	
Pretraining:	Epoch: [2][100/235]	Loss 0.0203 (0.0236)	
Pretraining:	Epoch: [2][110/235]	Loss 0.0204 (0.0234)	
Pretraining:	Epoch: [2][120/235]	Loss 0.0215 (0.0233)	
Pretraining:	Epoch: [2][130/235]	Loss 0.0207 (0.0232)	
Pretraining:	Epoch: [2][140/235]	Loss 0.0205 (0.0230)	
Pretraining:	Epoch: [2][150/235]	Loss 0.0217 (0.0230)	
Pretraining:	Epoch: [2][160/235]	Loss 0.0197 (0.0228)	
Pretraining:	Epoch: [2][170/235]	Loss 0.0197 (0.0227)	
Pretraining:	Epoch: [2][180/235]	Loss 0.0214 (0.0226)	
Pretraining:	Epoch: [2][190/235]	Loss 0.0211 (0.0225)	
Pretraining:	Epoch: [2][200/235]	Loss 0.0196 (0.0224)	
Pretraining:	Epoch: [2][210/235]	Loss 0.0201 (0.0223)	
Pretraining:	Epoch: [2][220/235]	Loss 0.0198 (0.0222)	
Pretraining:	Epoch: [2][230/235]	Loss 0.0195 (0.0221)	
Pretraining:	 Loss: 0.0220

Pretraining:	Epoch 3/20
----------
Pretraining:	Epoch: [3][10/235]	Loss 0.0181 (0.0193)	
Pretraining:	Epoch: [3][20/235]	Loss 0.0191 (0.0193)	
Pretraining:	Epoch: [3][30/235]	Loss 0.0188 (0.0192)	
Pretraining:	Epoch: [3][40/235]	Loss 0.0194 (0.0193)	
Pretraining:	Epoch: [3][50/235]	Loss 0.0206 (0.0192)	
Pretraining:	Epoch: [3][60/235]	Loss 0.0193 (0.0193)	
Pretraining:	Epoch: [3][70/235]	Loss 0.0185 (0.0193)	
Pretraining:	Epoch: [3][80/235]	Loss 0.0171 (0.0192)	
Pretraining:	Epoch: [3][90/235]	Loss 0.0196 (0.0192)	
Pretraining:	Epoch: [3][100/235]	Loss 0.0176 (0.0191)	
Pretraining:	Epoch: [3][110/235]	Loss 0.0176 (0.0191)	
Pretraining:	Epoch: [3][120/235]	Loss 0.0186 (0.0191)	
Pretraining:	Epoch: [3][130/235]	Loss 0.0179 (0.0191)	
Pretraining:	Epoch: [3][140/235]	Loss 0.0176 (0.0190)	
Pretraining:	Epoch: [3][150/235]	Loss 0.0192 (0.0190)	
Pretraining:	Epoch: [3][160/235]	Loss 0.0168 (0.0189)	
Pretraining:	Epoch: [3][170/235]	Loss 0.0172 (0.0189)	
Pretraining:	Epoch: [3][180/235]	Loss 0.0189 (0.0189)	
Pretraining:	Epoch: [3][190/235]	Loss 0.0185 (0.0188)	
Pretraining:	Epoch: [3][200/235]	Loss 0.0171 (0.0188)	
Pretraining:	Epoch: [3][210/235]	Loss 0.0177 (0.0187)	
Pretraining:	Epoch: [3][220/235]	Loss 0.0177 (0.0187)	
Pretraining:	Epoch: [3][230/235]	Loss 0.0175 (0.0186)	
Pretraining:	 Loss: 0.0186

Pretraining:	Epoch 4/20
----------
Pretraining:	Epoch: [4][10/235]	Loss 0.0161 (0.0172)	
Pretraining:	Epoch: [4][20/235]	Loss 0.0174 (0.0173)	
Pretraining:	Epoch: [4][30/235]	Loss 0.0168 (0.0173)	
Pretraining:	Epoch: [4][40/235]	Loss 0.0175 (0.0174)	
Pretraining:	Epoch: [4][50/235]	Loss 0.0185 (0.0173)	
Pretraining:	Epoch: [4][60/235]	Loss 0.0175 (0.0173)	
Pretraining:	Epoch: [4][70/235]	Loss 0.0168 (0.0174)	
Pretraining:	Epoch: [4][80/235]	Loss 0.0155 (0.0173)	
Pretraining:	Epoch: [4][90/235]	Loss 0.0178 (0.0173)	
Pretraining:	Epoch: [4][100/235]	Loss 0.0162 (0.0173)	
Pretraining:	Epoch: [4][110/235]	Loss 0.0161 (0.0173)	
Pretraining:	Epoch: [4][120/235]	Loss 0.0169 (0.0173)	
Pretraining:	Epoch: [4][130/235]	Loss 0.0166 (0.0173)	
Pretraining:	Epoch: [4][140/235]	Loss 0.0160 (0.0172)	
Pretraining:	Epoch: [4][150/235]	Loss 0.0179 (0.0173)	
Pretraining:	Epoch: [4][160/235]	Loss 0.0153 (0.0172)	
Pretraining:	Epoch: [4][170/235]	Loss 0.0159 (0.0172)	
Pretraining:	Epoch: [4][180/235]	Loss 0.0177 (0.0172)	
Pretraining:	Epoch: [4][190/235]	Loss 0.0173 (0.0172)	
Pretraining:	Epoch: [4][200/235]	Loss 0.0157 (0.0172)	
Pretraining:	Epoch: [4][210/235]	Loss 0.0165 (0.0171)	
Pretraining:	Epoch: [4][220/235]	Loss 0.0167 (0.0171)	
Pretraining:	Epoch: [4][230/235]	Loss 0.0162 (0.0170)	
Pretraining:	 Loss: 0.0170

Pretraining:	Epoch 5/20
----------
Pretraining:	Epoch: [5][10/235]	Loss 0.0153 (0.0161)	
Pretraining:	Epoch: [5][20/235]	Loss 0.0164 (0.0162)	
Pretraining:	Epoch: [5][30/235]	Loss 0.0157 (0.0161)	
Pretraining:	Epoch: [5][40/235]	Loss 0.0163 (0.0163)	
Pretraining:	Epoch: [5][50/235]	Loss 0.0173 (0.0162)	
Pretraining:	Epoch: [5][60/235]	Loss 0.0163 (0.0162)	
Pretraining:	Epoch: [5][70/235]	Loss 0.0158 (0.0162)	
Pretraining:	Epoch: [5][80/235]	Loss 0.0146 (0.0162)	
Pretraining:	Epoch: [5][90/235]	Loss 0.0163 (0.0162)	
Pretraining:	Epoch: [5][100/235]	Loss 0.0154 (0.0162)	
Pretraining:	Epoch: [5][110/235]	Loss 0.0152 (0.0162)	
Pretraining:	Epoch: [5][120/235]	Loss 0.0159 (0.0162)	
Pretraining:	Epoch: [5][130/235]	Loss 0.0156 (0.0162)	
Pretraining:	Epoch: [5][140/235]	Loss 0.0151 (0.0161)	
Pretraining:	Epoch: [5][150/235]	Loss 0.0170 (0.0162)	
Pretraining:	Epoch: [5][160/235]	Loss 0.0145 (0.0161)	
Pretraining:	Epoch: [5][170/235]	Loss 0.0150 (0.0161)	
Pretraining:	Epoch: [5][180/235]	Loss 0.0167 (0.0161)	
Pretraining:	Epoch: [5][190/235]	Loss 0.0164 (0.0161)	
Pretraining:	Epoch: [5][200/235]	Loss 0.0148 (0.0161)	
Pretraining:	Epoch: [5][210/235]	Loss 0.0157 (0.0161)	
Pretraining:	Epoch: [5][220/235]	Loss 0.0157 (0.0161)	
Pretraining:	Epoch: [5][230/235]	Loss 0.0154 (0.0160)	
Pretraining:	 Loss: 0.0160

Pretraining:	Epoch 6/20
----------
Pretraining:	Epoch: [6][10/235]	Loss 0.0143 (0.0153)	
Pretraining:	Epoch: [6][20/235]	Loss 0.0157 (0.0153)	
Pretraining:	Epoch: [6][30/235]	Loss 0.0149 (0.0153)	
Pretraining:	Epoch: [6][40/235]	Loss 0.0155 (0.0155)	
Pretraining:	Epoch: [6][50/235]	Loss 0.0166 (0.0154)	
Pretraining:	Epoch: [6][60/235]	Loss 0.0158 (0.0154)	
Pretraining:	Epoch: [6][70/235]	Loss 0.0151 (0.0155)	
Pretraining:	Epoch: [6][80/235]	Loss 0.0138 (0.0154)	
Pretraining:	Epoch: [6][90/235]	Loss 0.0157 (0.0154)	
Pretraining:	Epoch: [6][100/235]	Loss 0.0148 (0.0154)	
Pretraining:	Epoch: [6][110/235]	Loss 0.0145 (0.0154)	
Pretraining:	Epoch: [6][120/235]	Loss 0.0151 (0.0154)	
Pretraining:	Epoch: [6][130/235]	Loss 0.0148 (0.0154)	
Pretraining:	Epoch: [6][140/235]	Loss 0.0144 (0.0154)	
Pretraining:	Epoch: [6][150/235]	Loss 0.0164 (0.0154)	
Pretraining:	Epoch: [6][160/235]	Loss 0.0139 (0.0154)	
Pretraining:	Epoch: [6][170/235]	Loss 0.0145 (0.0154)	
Pretraining:	Epoch: [6][180/235]	Loss 0.0160 (0.0154)	
Pretraining:	Epoch: [6][190/235]	Loss 0.0157 (0.0154)	
Pretraining:	Epoch: [6][200/235]	Loss 0.0141 (0.0154)	
Pretraining:	Epoch: [6][210/235]	Loss 0.0151 (0.0154)	
Pretraining:	Epoch: [6][220/235]	Loss 0.0151 (0.0154)	
Pretraining:	Epoch: [6][230/235]	Loss 0.0147 (0.0153)	
Pretraining:	 Loss: 0.0153

Pretraining:	Epoch 7/20
----------
Pretraining:	Epoch: [7][10/235]	Loss 0.0138 (0.0146)	
Pretraining:	Epoch: [7][20/235]	Loss 0.0151 (0.0147)	
Pretraining:	Epoch: [7][30/235]	Loss 0.0143 (0.0147)	
Pretraining:	Epoch: [7][40/235]	Loss 0.0148 (0.0149)	
Pretraining:	Epoch: [7][50/235]	Loss 0.0159 (0.0148)	
Pretraining:	Epoch: [7][60/235]	Loss 0.0150 (0.0148)	
Pretraining:	Epoch: [7][70/235]	Loss 0.0144 (0.0149)	
Pretraining:	Epoch: [7][80/235]	Loss 0.0132 (0.0148)	
Pretraining:	Epoch: [7][90/235]	Loss 0.0149 (0.0148)	
Pretraining:	Epoch: [7][100/235]	Loss 0.0142 (0.0148)	
Pretraining:	Epoch: [7][110/235]	Loss 0.0139 (0.0148)	
Pretraining:	Epoch: [7][120/235]	Loss 0.0145 (0.0148)	
Pretraining:	Epoch: [7][130/235]	Loss 0.0145 (0.0148)	
Pretraining:	Epoch: [7][140/235]	Loss 0.0139 (0.0148)	
Pretraining:	Epoch: [7][150/235]	Loss 0.0159 (0.0148)	
Pretraining:	Epoch: [7][160/235]	Loss 0.0134 (0.0148)	
Pretraining:	Epoch: [7][170/235]	Loss 0.0140 (0.0148)	
Pretraining:	Epoch: [7][180/235]	Loss 0.0154 (0.0148)	
Pretraining:	Epoch: [7][190/235]	Loss 0.0152 (0.0148)	
Pretraining:	Epoch: [7][200/235]	Loss 0.0136 (0.0148)	
Pretraining:	Epoch: [7][210/235]	Loss 0.0146 (0.0148)	
Pretraining:	Epoch: [7][220/235]	Loss 0.0145 (0.0148)	
Pretraining:	Epoch: [7][230/235]	Loss 0.0142 (0.0147)	
Pretraining:	 Loss: 0.0147

Pretraining:	Epoch 8/20
----------
Pretraining:	Epoch: [8][10/235]	Loss 0.0134 (0.0141)	
Pretraining:	Epoch: [8][20/235]	Loss 0.0147 (0.0142)	
Pretraining:	Epoch: [8][30/235]	Loss 0.0141 (0.0143)	
Pretraining:	Epoch: [8][40/235]	Loss 0.0149 (0.0145)	
Pretraining:	Epoch: [8][50/235]	Loss 0.0155 (0.0145)	
Pretraining:	Epoch: [8][60/235]	Loss 0.0146 (0.0145)	
Pretraining:	Epoch: [8][70/235]	Loss 0.0140 (0.0146)	
Pretraining:	Epoch: [8][80/235]	Loss 0.0129 (0.0145)	
Pretraining:	Epoch: [8][90/235]	Loss 0.0145 (0.0145)	
Pretraining:	Epoch: [8][100/235]	Loss 0.0138 (0.0145)	
Pretraining:	Epoch: [8][110/235]	Loss 0.0135 (0.0145)	
Pretraining:	Epoch: [8][120/235]	Loss 0.0140 (0.0145)	
Pretraining:	Epoch: [8][130/235]	Loss 0.0139 (0.0145)	
Pretraining:	Epoch: [8][140/235]	Loss 0.0134 (0.0144)	
Pretraining:	Epoch: [8][150/235]	Loss 0.0154 (0.0145)	
Pretraining:	Epoch: [8][160/235]	Loss 0.0130 (0.0144)	
Pretraining:	Epoch: [8][170/235]	Loss 0.0135 (0.0144)	
Pretraining:	Epoch: [8][180/235]	Loss 0.0149 (0.0144)	
Pretraining:	Epoch: [8][190/235]	Loss 0.0148 (0.0144)	
Pretraining:	Epoch: [8][200/235]	Loss 0.0131 (0.0144)	
Pretraining:	Epoch: [8][210/235]	Loss 0.0142 (0.0144)	
Pretraining:	Epoch: [8][220/235]	Loss 0.0142 (0.0144)	
Pretraining:	Epoch: [8][230/235]	Loss 0.0139 (0.0144)	
Pretraining:	 Loss: 0.0143

Pretraining:	Epoch 9/20
----------
Pretraining:	Epoch: [9][10/235]	Loss 0.0130 (0.0137)	
Pretraining:	Epoch: [9][20/235]	Loss 0.0144 (0.0138)	
Pretraining:	Epoch: [9][30/235]	Loss 0.0137 (0.0139)	
Pretraining:	Epoch: [9][40/235]	Loss 0.0139 (0.0140)	
Pretraining:	Epoch: [9][50/235]	Loss 0.0150 (0.0139)	
Pretraining:	Epoch: [9][60/235]	Loss 0.0141 (0.0140)	
Pretraining:	Epoch: [9][70/235]	Loss 0.0135 (0.0140)	
Pretraining:	Epoch: [9][80/235]	Loss 0.0124 (0.0140)	
Pretraining:	Epoch: [9][90/235]	Loss 0.0141 (0.0140)	
Pretraining:	Epoch: [9][100/235]	Loss 0.0133 (0.0139)	
Pretraining:	Epoch: [9][110/235]	Loss 0.0131 (0.0139)	
Pretraining:	Epoch: [9][120/235]	Loss 0.0136 (0.0139)	
Pretraining:	Epoch: [9][130/235]	Loss 0.0135 (0.0139)	
Pretraining:	Epoch: [9][140/235]	Loss 0.0131 (0.0139)	
Pretraining:	Epoch: [9][150/235]	Loss 0.0151 (0.0140)	
Pretraining:	Epoch: [9][160/235]	Loss 0.0127 (0.0139)	
Pretraining:	Epoch: [9][170/235]	Loss 0.0132 (0.0139)	
Pretraining:	Epoch: [9][180/235]	Loss 0.0146 (0.0139)	
Pretraining:	Epoch: [9][190/235]	Loss 0.0144 (0.0140)	
Pretraining:	Epoch: [9][200/235]	Loss 0.0127 (0.0140)	
Pretraining:	Epoch: [9][210/235]	Loss 0.0138 (0.0139)	
Pretraining:	Epoch: [9][220/235]	Loss 0.0138 (0.0139)	
Pretraining:	Epoch: [9][230/235]	Loss 0.0135 (0.0139)	
Pretraining:	 Loss: 0.0139

Pretraining:	Epoch 10/20
----------
Pretraining:	Epoch: [10][10/235]	Loss 0.0127 (0.0133)	
Pretraining:	Epoch: [10][20/235]	Loss 0.0139 (0.0134)	
Pretraining:	Epoch: [10][30/235]	Loss 0.0132 (0.0135)	
Pretraining:	Epoch: [10][40/235]	Loss 0.0136 (0.0136)	
Pretraining:	Epoch: [10][50/235]	Loss 0.0146 (0.0135)	
Pretraining:	Epoch: [10][60/235]	Loss 0.0137 (0.0136)	
Pretraining:	Epoch: [10][70/235]	Loss 0.0132 (0.0136)	
Pretraining:	Epoch: [10][80/235]	Loss 0.0121 (0.0136)	
Pretraining:	Epoch: [10][90/235]	Loss 0.0137 (0.0136)	
Pretraining:	Epoch: [10][100/235]	Loss 0.0130 (0.0135)	
Pretraining:	Epoch: [10][110/235]	Loss 0.0127 (0.0136)	
Pretraining:	Epoch: [10][120/235]	Loss 0.0133 (0.0136)	
Pretraining:	Epoch: [10][130/235]	Loss 0.0132 (0.0136)	
Pretraining:	Epoch: [10][140/235]	Loss 0.0127 (0.0135)	
Pretraining:	Epoch: [10][150/235]	Loss 0.0148 (0.0136)	
Pretraining:	Epoch: [10][160/235]	Loss 0.0124 (0.0136)	
Pretraining:	Epoch: [10][170/235]	Loss 0.0128 (0.0136)	
Pretraining:	Epoch: [10][180/235]	Loss 0.0143 (0.0136)	
Pretraining:	Epoch: [10][190/235]	Loss 0.0140 (0.0136)	
Pretraining:	Epoch: [10][200/235]	Loss 0.0124 (0.0136)	
Pretraining:	Epoch: [10][210/235]	Loss 0.0135 (0.0136)	
Pretraining:	Epoch: [10][220/235]	Loss 0.0135 (0.0136)	
Pretraining:	Epoch: [10][230/235]	Loss 0.0132 (0.0136)	
Pretraining:	 Loss: 0.0135

Pretraining:	Epoch 11/20
----------
Pretraining:	Epoch: [11][10/235]	Loss 0.0125 (0.0130)	
Pretraining:	Epoch: [11][20/235]	Loss 0.0135 (0.0131)	
Pretraining:	Epoch: [11][30/235]	Loss 0.0128 (0.0131)	
Pretraining:	Epoch: [11][40/235]	Loss 0.0133 (0.0133)	
Pretraining:	Epoch: [11][50/235]	Loss 0.0143 (0.0132)	
Pretraining:	Epoch: [11][60/235]	Loss 0.0134 (0.0133)	
Pretraining:	Epoch: [11][70/235]	Loss 0.0129 (0.0133)	
Pretraining:	Epoch: [11][80/235]	Loss 0.0119 (0.0132)	
Pretraining:	Epoch: [11][90/235]	Loss 0.0134 (0.0133)	
Pretraining:	Epoch: [11][100/235]	Loss 0.0127 (0.0132)	
Pretraining:	Epoch: [11][110/235]	Loss 0.0124 (0.0132)	
Pretraining:	Epoch: [11][120/235]	Loss 0.0131 (0.0133)	
Pretraining:	Epoch: [11][130/235]	Loss 0.0130 (0.0133)	
Pretraining:	Epoch: [11][140/235]	Loss 0.0125 (0.0132)	
Pretraining:	Epoch: [11][150/235]	Loss 0.0145 (0.0133)	
Pretraining:	Epoch: [11][160/235]	Loss 0.0121 (0.0133)	
Pretraining:	Epoch: [11][170/235]	Loss 0.0125 (0.0133)	
Pretraining:	Epoch: [11][180/235]	Loss 0.0140 (0.0133)	
Pretraining:	Epoch: [11][190/235]	Loss 0.0138 (0.0133)	
Pretraining:	Epoch: [11][200/235]	Loss 0.0121 (0.0133)	
Pretraining:	Epoch: [11][210/235]	Loss 0.0132 (0.0133)	
Pretraining:	Epoch: [11][220/235]	Loss 0.0132 (0.0133)	
Pretraining:	Epoch: [11][230/235]	Loss 0.0129 (0.0133)	
Pretraining:	 Loss: 0.0132

Pretraining:	Epoch 12/20
----------
Pretraining:	Epoch: [12][10/235]	Loss 0.0122 (0.0127)	
Pretraining:	Epoch: [12][20/235]	Loss 0.0132 (0.0128)	
Pretraining:	Epoch: [12][30/235]	Loss 0.0125 (0.0129)	
Pretraining:	Epoch: [12][40/235]	Loss 0.0130 (0.0130)	
Pretraining:	Epoch: [12][50/235]	Loss 0.0140 (0.0129)	
Pretraining:	Epoch: [12][60/235]	Loss 0.0133 (0.0130)	
Pretraining:	Epoch: [12][70/235]	Loss 0.0127 (0.0130)	
Pretraining:	Epoch: [12][80/235]	Loss 0.0117 (0.0130)	
Pretraining:	Epoch: [12][90/235]	Loss 0.0133 (0.0130)	
Pretraining:	Epoch: [12][100/235]	Loss 0.0125 (0.0130)	
Pretraining:	Epoch: [12][110/235]	Loss 0.0121 (0.0130)	
Pretraining:	Epoch: [12][120/235]	Loss 0.0128 (0.0130)	
Pretraining:	Epoch: [12][130/235]	Loss 0.0127 (0.0130)	
Pretraining:	Epoch: [12][140/235]	Loss 0.0122 (0.0130)	
Pretraining:	Epoch: [12][150/235]	Loss 0.0142 (0.0130)	
Pretraining:	Epoch: [12][160/235]	Loss 0.0119 (0.0130)	
Pretraining:	Epoch: [12][170/235]	Loss 0.0123 (0.0130)	
Pretraining:	Epoch: [12][180/235]	Loss 0.0137 (0.0130)	
Pretraining:	Epoch: [12][190/235]	Loss 0.0136 (0.0131)	
Pretraining:	Epoch: [12][200/235]	Loss 0.0119 (0.0131)	
Pretraining:	Epoch: [12][210/235]	Loss 0.0130 (0.0131)	
Pretraining:	Epoch: [12][220/235]	Loss 0.0130 (0.0130)	
Pretraining:	Epoch: [12][230/235]	Loss 0.0127 (0.0130)	
Pretraining:	 Loss: 0.0130

Pretraining:	Epoch 13/20
----------
Pretraining:	Epoch: [13][10/235]	Loss 0.0119 (0.0125)	
Pretraining:	Epoch: [13][20/235]	Loss 0.0129 (0.0126)	
Pretraining:	Epoch: [13][30/235]	Loss 0.0123 (0.0126)	
Pretraining:	Epoch: [13][40/235]	Loss 0.0128 (0.0127)	
Pretraining:	Epoch: [13][50/235]	Loss 0.0138 (0.0127)	
Pretraining:	Epoch: [13][60/235]	Loss 0.0130 (0.0127)	
Pretraining:	Epoch: [13][70/235]	Loss 0.0124 (0.0128)	
Pretraining:	Epoch: [13][80/235]	Loss 0.0115 (0.0127)	
Pretraining:	Epoch: [13][90/235]	Loss 0.0130 (0.0128)	
Pretraining:	Epoch: [13][100/235]	Loss 0.0121 (0.0127)	
Pretraining:	Epoch: [13][110/235]	Loss 0.0119 (0.0128)	
Pretraining:	Epoch: [13][120/235]	Loss 0.0125 (0.0128)	
Pretraining:	Epoch: [13][130/235]	Loss 0.0125 (0.0128)	
Pretraining:	Epoch: [13][140/235]	Loss 0.0120 (0.0127)	
Pretraining:	Epoch: [13][150/235]	Loss 0.0140 (0.0128)	
Pretraining:	Epoch: [13][160/235]	Loss 0.0117 (0.0128)	
Pretraining:	Epoch: [13][170/235]	Loss 0.0121 (0.0128)	
Pretraining:	Epoch: [13][180/235]	Loss 0.0135 (0.0128)	
Pretraining:	Epoch: [13][190/235]	Loss 0.0134 (0.0128)	
Pretraining:	Epoch: [13][200/235]	Loss 0.0117 (0.0128)	
Pretraining:	Epoch: [13][210/235]	Loss 0.0128 (0.0128)	
Pretraining:	Epoch: [13][220/235]	Loss 0.0127 (0.0128)	
Pretraining:	Epoch: [13][230/235]	Loss 0.0124 (0.0128)	
Pretraining:	 Loss: 0.0128

Pretraining:	Epoch 14/20
----------
Pretraining:	Epoch: [14][10/235]	Loss 0.0118 (0.0123)	
Pretraining:	Epoch: [14][20/235]	Loss 0.0127 (0.0124)	
Pretraining:	Epoch: [14][30/235]	Loss 0.0120 (0.0124)	
Pretraining:	Epoch: [14][40/235]	Loss 0.0126 (0.0125)	
Pretraining:	Epoch: [14][50/235]	Loss 0.0136 (0.0125)	
Pretraining:	Epoch: [14][60/235]	Loss 0.0129 (0.0125)	
Pretraining:	Epoch: [14][70/235]	Loss 0.0122 (0.0126)	
Pretraining:	Epoch: [14][80/235]	Loss 0.0113 (0.0125)	
Pretraining:	Epoch: [14][90/235]	Loss 0.0128 (0.0126)	
Pretraining:	Epoch: [14][100/235]	Loss 0.0120 (0.0125)	
Pretraining:	Epoch: [14][110/235]	Loss 0.0117 (0.0126)	
Pretraining:	Epoch: [14][120/235]	Loss 0.0123 (0.0126)	
Pretraining:	Epoch: [14][130/235]	Loss 0.0124 (0.0126)	
Pretraining:	Epoch: [14][140/235]	Loss 0.0119 (0.0126)	
Pretraining:	Epoch: [14][150/235]	Loss 0.0138 (0.0126)	
Pretraining:	Epoch: [14][160/235]	Loss 0.0115 (0.0126)	
Pretraining:	Epoch: [14][170/235]	Loss 0.0119 (0.0126)	
Pretraining:	Epoch: [14][180/235]	Loss 0.0133 (0.0126)	
Pretraining:	Epoch: [14][190/235]	Loss 0.0132 (0.0126)	
Pretraining:	Epoch: [14][200/235]	Loss 0.0116 (0.0126)	
Pretraining:	Epoch: [14][210/235]	Loss 0.0126 (0.0126)	
Pretraining:	Epoch: [14][220/235]	Loss 0.0125 (0.0126)	
Pretraining:	Epoch: [14][230/235]	Loss 0.0124 (0.0126)	
Pretraining:	 Loss: 0.0126

Pretraining:	Epoch 15/20
----------
Pretraining:	Epoch: [15][10/235]	Loss 0.0115 (0.0121)	
Pretraining:	Epoch: [15][20/235]	Loss 0.0126 (0.0122)	
Pretraining:	Epoch: [15][30/235]	Loss 0.0118 (0.0122)	
Pretraining:	Epoch: [15][40/235]	Loss 0.0123 (0.0123)	
Pretraining:	Epoch: [15][50/235]	Loss 0.0134 (0.0123)	
Pretraining:	Epoch: [15][60/235]	Loss 0.0127 (0.0124)	
Pretraining:	Epoch: [15][70/235]	Loss 0.0121 (0.0124)	
Pretraining:	Epoch: [15][80/235]	Loss 0.0111 (0.0124)	
Pretraining:	Epoch: [15][90/235]	Loss 0.0130 (0.0124)	
Pretraining:	Epoch: [15][100/235]	Loss 0.0119 (0.0124)	
Pretraining:	Epoch: [15][110/235]	Loss 0.0116 (0.0124)	
Pretraining:	Epoch: [15][120/235]	Loss 0.0122 (0.0124)	
Pretraining:	Epoch: [15][130/235]	Loss 0.0122 (0.0125)	
Pretraining:	Epoch: [15][140/235]	Loss 0.0118 (0.0124)	
Pretraining:	Epoch: [15][150/235]	Loss 0.0137 (0.0125)	
Pretraining:	Epoch: [15][160/235]	Loss 0.0113 (0.0124)	
Pretraining:	Epoch: [15][170/235]	Loss 0.0117 (0.0124)	
Pretraining:	Epoch: [15][180/235]	Loss 0.0131 (0.0124)	
Pretraining:	Epoch: [15][190/235]	Loss 0.0131 (0.0125)	
Pretraining:	Epoch: [15][200/235]	Loss 0.0114 (0.0125)	
Pretraining:	Epoch: [15][210/235]	Loss 0.0124 (0.0125)	
Pretraining:	Epoch: [15][220/235]	Loss 0.0123 (0.0125)	
Pretraining:	Epoch: [15][230/235]	Loss 0.0121 (0.0125)	
Pretraining:	 Loss: 0.0124

Pretraining:	Epoch 16/20
----------
Pretraining:	Epoch: [16][10/235]	Loss 0.0114 (0.0120)	
Pretraining:	Epoch: [16][20/235]	Loss 0.0124 (0.0121)	
Pretraining:	Epoch: [16][30/235]	Loss 0.0116 (0.0121)	
Pretraining:	Epoch: [16][40/235]	Loss 0.0122 (0.0122)	
Pretraining:	Epoch: [16][50/235]	Loss 0.0132 (0.0121)	
Pretraining:	Epoch: [16][60/235]	Loss 0.0125 (0.0122)	
Pretraining:	Epoch: [16][70/235]	Loss 0.0119 (0.0122)	
Pretraining:	Epoch: [16][80/235]	Loss 0.0111 (0.0122)	
Pretraining:	Epoch: [16][90/235]	Loss 0.0126 (0.0122)	
Pretraining:	Epoch: [16][100/235]	Loss 0.0117 (0.0122)	
Pretraining:	Epoch: [16][110/235]	Loss 0.0115 (0.0122)	
Pretraining:	Epoch: [16][120/235]	Loss 0.0121 (0.0123)	
Pretraining:	Epoch: [16][130/235]	Loss 0.0120 (0.0123)	
Pretraining:	Epoch: [16][140/235]	Loss 0.0115 (0.0123)	
Pretraining:	Epoch: [16][150/235]	Loss 0.0135 (0.0123)	
Pretraining:	Epoch: [16][160/235]	Loss 0.0112 (0.0123)	
Pretraining:	Epoch: [16][170/235]	Loss 0.0116 (0.0123)	
Pretraining:	Epoch: [16][180/235]	Loss 0.0129 (0.0123)	
Pretraining:	Epoch: [16][190/235]	Loss 0.0130 (0.0123)	
Pretraining:	Epoch: [16][200/235]	Loss 0.0112 (0.0123)	
Pretraining:	Epoch: [16][210/235]	Loss 0.0123 (0.0123)	
Pretraining:	Epoch: [16][220/235]	Loss 0.0122 (0.0123)	
Pretraining:	Epoch: [16][230/235]	Loss 0.0120 (0.0123)	
Pretraining:	 Loss: 0.0123

Pretraining:	Epoch 17/20
----------
Pretraining:	Epoch: [17][10/235]	Loss 0.0112 (0.0119)	
Pretraining:	Epoch: [17][20/235]	Loss 0.0123 (0.0120)	
Pretraining:	Epoch: [17][30/235]	Loss 0.0115 (0.0120)	
Pretraining:	Epoch: [17][40/235]	Loss 0.0121 (0.0121)	
Pretraining:	Epoch: [17][50/235]	Loss 0.0130 (0.0121)	
Pretraining:	Epoch: [17][60/235]	Loss 0.0123 (0.0121)	
Pretraining:	Epoch: [17][70/235]	Loss 0.0120 (0.0122)	
Pretraining:	Epoch: [17][80/235]	Loss 0.0110 (0.0121)	
Pretraining:	Epoch: [17][90/235]	Loss 0.0128 (0.0122)	
Pretraining:	Epoch: [17][100/235]	Loss 0.0115 (0.0122)	
Pretraining:	Epoch: [17][110/235]	Loss 0.0114 (0.0122)	
Pretraining:	Epoch: [17][120/235]	Loss 0.0118 (0.0122)	
Pretraining:	Epoch: [17][130/235]	Loss 0.0119 (0.0122)	
Pretraining:	Epoch: [17][140/235]	Loss 0.0113 (0.0122)	
Pretraining:	Epoch: [17][150/235]	Loss 0.0133 (0.0122)	
Pretraining:	Epoch: [17][160/235]	Loss 0.0110 (0.0122)	
Pretraining:	Epoch: [17][170/235]	Loss 0.0114 (0.0122)	
Pretraining:	Epoch: [17][180/235]	Loss 0.0126 (0.0121)	
Pretraining:	Epoch: [17][190/235]	Loss 0.0125 (0.0122)	
Pretraining:	Epoch: [17][200/235]	Loss 0.0111 (0.0122)	
Pretraining:	Epoch: [17][210/235]	Loss 0.0123 (0.0122)	
Pretraining:	Epoch: [17][220/235]	Loss 0.0121 (0.0122)	
Pretraining:	Epoch: [17][230/235]	Loss 0.0119 (0.0121)	
Pretraining:	 Loss: 0.0121

Pretraining:	Epoch 18/20
----------
Pretraining:	Epoch: [18][10/235]	Loss 0.0111 (0.0117)	
Pretraining:	Epoch: [18][20/235]	Loss 0.0120 (0.0118)	
Pretraining:	Epoch: [18][30/235]	Loss 0.0115 (0.0118)	
Pretraining:	Epoch: [18][40/235]	Loss 0.0120 (0.0119)	
Pretraining:	Epoch: [18][50/235]	Loss 0.0129 (0.0118)	
Pretraining:	Epoch: [18][60/235]	Loss 0.0120 (0.0119)	
Pretraining:	Epoch: [18][70/235]	Loss 0.0117 (0.0119)	
Pretraining:	Epoch: [18][80/235]	Loss 0.0107 (0.0119)	
Pretraining:	Epoch: [18][90/235]	Loss 0.0124 (0.0119)	
Pretraining:	Epoch: [18][100/235]	Loss 0.0113 (0.0119)	
Pretraining:	Epoch: [18][110/235]	Loss 0.0110 (0.0119)	
Pretraining:	Epoch: [18][120/235]	Loss 0.0117 (0.0119)	
Pretraining:	Epoch: [18][130/235]	Loss 0.0116 (0.0119)	
Pretraining:	Epoch: [18][140/235]	Loss 0.0112 (0.0119)	
Pretraining:	Epoch: [18][150/235]	Loss 0.0131 (0.0119)	
Pretraining:	Epoch: [18][160/235]	Loss 0.0108 (0.0119)	
Pretraining:	Epoch: [18][170/235]	Loss 0.0112 (0.0119)	
Pretraining:	Epoch: [18][180/235]	Loss 0.0125 (0.0119)	
Pretraining:	Epoch: [18][190/235]	Loss 0.0123 (0.0119)	
Pretraining:	Epoch: [18][200/235]	Loss 0.0110 (0.0120)	
Pretraining:	Epoch: [18][210/235]	Loss 0.0120 (0.0119)	
Pretraining:	Epoch: [18][220/235]	Loss 0.0120 (0.0119)	
Pretraining:	Epoch: [18][230/235]	Loss 0.0117 (0.0119)	
Pretraining:	 Loss: 0.0119

Pretraining:	Epoch 19/20
----------
Pretraining:	Epoch: [19][10/235]	Loss 0.0109 (0.0115)	
Pretraining:	Epoch: [19][20/235]	Loss 0.0118 (0.0116)	
Pretraining:	Epoch: [19][30/235]	Loss 0.0112 (0.0116)	
Pretraining:	Epoch: [19][40/235]	Loss 0.0117 (0.0117)	
Pretraining:	Epoch: [19][50/235]	Loss 0.0126 (0.0116)	
Pretraining:	Epoch: [19][60/235]	Loss 0.0119 (0.0117)	
Pretraining:	Epoch: [19][70/235]	Loss 0.0115 (0.0117)	
Pretraining:	Epoch: [19][80/235]	Loss 0.0105 (0.0117)	
Pretraining:	Epoch: [19][90/235]	Loss 0.0120 (0.0117)	
Pretraining:	Epoch: [19][100/235]	Loss 0.0111 (0.0117)	
Pretraining:	Epoch: [19][110/235]	Loss 0.0108 (0.0117)	
Pretraining:	Epoch: [19][120/235]	Loss 0.0115 (0.0117)	
Pretraining:	Epoch: [19][130/235]	Loss 0.0114 (0.0117)	
Pretraining:	Epoch: [19][140/235]	Loss 0.0110 (0.0117)	
Pretraining:	Epoch: [19][150/235]	Loss 0.0130 (0.0117)	
Pretraining:	Epoch: [19][160/235]	Loss 0.0107 (0.0117)	
Pretraining:	Epoch: [19][170/235]	Loss 0.0111 (0.0117)	
Pretraining:	Epoch: [19][180/235]	Loss 0.0124 (0.0117)	
Pretraining:	Epoch: [19][190/235]	Loss 0.0122 (0.0117)	
Pretraining:	Epoch: [19][200/235]	Loss 0.0108 (0.0118)	
Pretraining:	Epoch: [19][210/235]	Loss 0.0118 (0.0118)	
Pretraining:	Epoch: [19][220/235]	Loss 0.0118 (0.0117)	
Pretraining:	Epoch: [19][230/235]	Loss 0.0115 (0.0117)	
Pretraining:	 Loss: 0.0117

Pretraining:	Epoch 20/20
----------
Pretraining:	Epoch: [20][10/235]	Loss 0.0107 (0.0113)	
Pretraining:	Epoch: [20][20/235]	Loss 0.0116 (0.0114)	
Pretraining:	Epoch: [20][30/235]	Loss 0.0109 (0.0114)	
Pretraining:	Epoch: [20][40/235]	Loss 0.0116 (0.0115)	
Pretraining:	Epoch: [20][50/235]	Loss 0.0124 (0.0114)	
Pretraining:	Epoch: [20][60/235]	Loss 0.0117 (0.0115)	
Pretraining:	Epoch: [20][70/235]	Loss 0.0113 (0.0115)	
Pretraining:	Epoch: [20][80/235]	Loss 0.0103 (0.0115)	
Pretraining:	Epoch: [20][90/235]	Loss 0.0118 (0.0115)	
Pretraining:	Epoch: [20][100/235]	Loss 0.0110 (0.0115)	
Pretraining:	Epoch: [20][110/235]	Loss 0.0106 (0.0115)	
Pretraining:	Epoch: [20][120/235]	Loss 0.0113 (0.0115)	
Pretraining:	Epoch: [20][130/235]	Loss 0.0113 (0.0115)	
Pretraining:	Epoch: [20][140/235]	Loss 0.0108 (0.0115)	
Pretraining:	Epoch: [20][150/235]	Loss 0.0128 (0.0115)	
Pretraining:	Epoch: [20][160/235]	Loss 0.0106 (0.0115)	
Pretraining:	Epoch: [20][170/235]	Loss 0.0110 (0.0115)	
Pretraining:	Epoch: [20][180/235]	Loss 0.0122 (0.0115)	
Pretraining:	Epoch: [20][190/235]	Loss 0.0120 (0.0116)	
Pretraining:	Epoch: [20][200/235]	Loss 0.0107 (0.0116)	
Pretraining:	Epoch: [20][210/235]	Loss 0.0116 (0.0116)	
Pretraining:	Epoch: [20][220/235]	Loss 0.0115 (0.0116)	
Pretraining:	Epoch: [20][230/235]	Loss 0.0113 (0.0116)	
Pretraining:	 Loss: 0.0116

Pretraining complete in 0m 50s

Initializing cluster centers based on K-means

Begin clusters training

Updating target distribution
NMI: 0.74255	ARI: 0.68582	Acc 0.80833

Epoch 1/1000
----------
Epoch: [1][10/235]	Loss 0.0266 (0.0303)	Loss_recovery 0.0139 (0.0150)	Loss clustering 0.0127 (0.0152)	
Epoch: [1][20/235]	Loss 0.0257 (0.0285)	Loss_recovery 0.0138 (0.0145)	Loss clustering 0.0119 (0.0139)	
Epoch: [1][30/235]	Loss 0.0242 (0.0271)	Loss_recovery 0.0128 (0.0141)	Loss clustering 0.0114 (0.0130)	
Epoch: [1][40/235]	Loss 0.0225 (0.0262)	Loss_recovery 0.0134 (0.0140)	Loss clustering 0.0091 (0.0122)	
Epoch: [1][50/235]	Loss 0.0230 (0.0255)	Loss_recovery 0.0142 (0.0138)	Loss clustering 0.0089 (0.0116)	
Epoch: [1][60/235]	Loss 0.0229 (0.0249)	Loss_recovery 0.0135 (0.0138)	Loss clustering 0.0094 (0.0111)	
Epoch: [1][70/235]	Loss 0.0207 (0.0244)	Loss_recovery 0.0128 (0.0137)	Loss clustering 0.0080 (0.0107)	
Epoch: [1][80/235]	Loss 0.0204 (0.0240)	Loss_recovery 0.0122 (0.0136)	Loss clustering 0.0083 (0.0104)	

Updating target distribution:
NMI: 0.76430	ARI: 0.70587	Acc 0.81810	
Epoch: [1][90/235]	Loss 0.0397 (0.0257)	Loss_recovery 0.0145 (0.0137)	Loss clustering 0.0252 (0.0121)	
Epoch: [1][100/235]	Loss 0.0378 (0.0269)	Loss_recovery 0.0146 (0.0138)	Loss clustering 0.0232 (0.0131)	
Epoch: [1][110/235]	Loss 0.0383 (0.0279)	Loss_recovery 0.0145 (0.0140)	Loss clustering 0.0239 (0.0139)	
Epoch: [1][120/235]	Loss 0.0339 (0.0285)	Loss_recovery 0.0154 (0.0141)	Loss clustering 0.0185 (0.0144)	
Epoch: [1][130/235]	Loss 0.0345 (0.0290)	Loss_recovery 0.0154 (0.0142)	Loss clustering 0.0191 (0.0148)	
Epoch: [1][140/235]	Loss 0.0341 (0.0294)	Loss_recovery 0.0149 (0.0143)	Loss clustering 0.0192 (0.0151)	
Epoch: [1][150/235]	Loss 0.0360 (0.0299)	Loss_recovery 0.0171 (0.0144)	Loss clustering 0.0189 (0.0154)	
Epoch: [1][160/235]	Loss 0.0348 (0.0302)	Loss_recovery 0.0148 (0.0145)	Loss clustering 0.0201 (0.0156)	

Updating target distribution:
NMI: 0.78369	ARI: 0.72530	Acc 0.82678	
Epoch: [1][170/235]	Loss 0.0452 (0.0310)	Loss_recovery 0.0160 (0.0146)	Loss clustering 0.0292 (0.0164)	
Epoch: [1][180/235]	Loss 0.0430 (0.0317)	Loss_recovery 0.0170 (0.0147)	Loss clustering 0.0260 (0.0170)	
Epoch: [1][190/235]	Loss 0.0438 (0.0324)	Loss_recovery 0.0170 (0.0148)	Loss clustering 0.0268 (0.0175)	
Epoch: [1][200/235]	Loss 0.0421 (0.0329)	Loss_recovery 0.0155 (0.0150)	Loss clustering 0.0267 (0.0180)	
Epoch: [1][210/235]	Loss 0.0427 (0.0334)	Loss_recovery 0.0167 (0.0150)	Loss clustering 0.0260 (0.0183)	
Epoch: [1][220/235]	Loss 0.0422 (0.0338)	Loss_recovery 0.0165 (0.0151)	Loss clustering 0.0256 (0.0187)	
Epoch: [1][230/235]	Loss 0.0423 (0.0342)	Loss_recovery 0.0167 (0.0152)	Loss clustering 0.0256 (0.0190)	
Loss: 0.0343	Loss_recovery: 0.0152	Loss_clustering: 0.0191

Epoch 2/1000
----------

Updating target distribution:
NMI: 0.79038	ARI: 0.73242	Acc 0.82898	
Epoch: [2][10/235]	Loss 0.0459 (0.0465)	Loss_recovery 0.0161 (0.0166)	Loss clustering 0.0297 (0.0299)	
Epoch: [2][20/235]	Loss 0.0454 (0.0463)	Loss_recovery 0.0166 (0.0167)	Loss clustering 0.0288 (0.0297)	
Epoch: [2][30/235]	Loss 0.0455 (0.0460)	Loss_recovery 0.0167 (0.0166)	Loss clustering 0.0289 (0.0294)	
Epoch: [2][40/235]	Loss 0.0455 (0.0459)	Loss_recovery 0.0169 (0.0168)	Loss clustering 0.0287 (0.0292)	
Epoch: [2][50/235]	Loss 0.0470 (0.0457)	Loss_recovery 0.0180 (0.0167)	Loss clustering 0.0290 (0.0290)	
Epoch: [2][60/235]	Loss 0.0454 (0.0455)	Loss_recovery 0.0171 (0.0167)	Loss clustering 0.0283 (0.0287)	
Epoch: [2][70/235]	Loss 0.0428 (0.0453)	Loss_recovery 0.0162 (0.0168)	Loss clustering 0.0266 (0.0285)	
Epoch: [2][80/235]	Loss 0.0429 (0.0451)	Loss_recovery 0.0156 (0.0168)	Loss clustering 0.0273 (0.0284)	

Updating target distribution:
NMI: 0.80054	ARI: 0.74242	Acc 0.83363	
Epoch: [2][90/235]	Loss 0.0491 (0.0455)	Loss_recovery 0.0172 (0.0168)	Loss clustering 0.0319 (0.0287)	
Epoch: [2][100/235]	Loss 0.0458 (0.0456)	Loss_recovery 0.0158 (0.0168)	Loss clustering 0.0301 (0.0288)	
Epoch: [2][110/235]	Loss 0.0462 (0.0457)	Loss_recovery 0.0156 (0.0168)	Loss clustering 0.0306 (0.0289)	
Epoch: [2][120/235]	Loss 0.0453 (0.0458)	Loss_recovery 0.0162 (0.0169)	Loss clustering 0.0291 (0.0289)	
Epoch: [2][130/235]	Loss 0.0456 (0.0458)	Loss_recovery 0.0165 (0.0169)	Loss clustering 0.0291 (0.0289)	
Epoch: [2][140/235]	Loss 0.0449 (0.0458)	Loss_recovery 0.0160 (0.0168)	Loss clustering 0.0289 (0.0290)	
Epoch: [2][150/235]	Loss 0.0469 (0.0459)	Loss_recovery 0.0182 (0.0169)	Loss clustering 0.0287 (0.0290)	
Epoch: [2][160/235]	Loss 0.0461 (0.0459)	Loss_recovery 0.0161 (0.0169)	Loss clustering 0.0299 (0.0290)	

Updating target distribution:
NMI: 0.80646	ARI: 0.74860	Acc 0.83600	
Label divergence 0.009283333333333333< tol 0.01
Reached tolerance threshold. Stopping training.
Training complete in 1m 13s
